{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71f3e58a-798c-4e0e-848d-ae34cbbe6284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "'''ResNet in PyTorch.\n",
    "\n",
    "For Pre-activation ResNet, see 'preact_resnet.py'.\n",
    "\n",
    "Reference:\n",
    "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, prob_dropout=0.3):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        # mod here\n",
    "        # self.dropout1 = nn.Dropout2d(p=prob_dropout)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "\n",
    "        # mod here\n",
    "        # self.dropout2 = nn.Dropout2d(p=prob_dropout)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
    "\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
    "\n",
    "\n",
    "def test():\n",
    "    net = ResNet18()\n",
    "    y = net(torch.randn(1, 3, 32, 32))\n",
    "    print(y.size())\n",
    "\n",
    "# test()\n",
    "\n",
    "\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e602daa2-c0f2-4320-b0ec-ccaea77ec1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(1024)\n",
    "\n",
    "def load_cifar_batch1(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "# Specify the folder where the CIFAR-10 batch files are\n",
    "cifar10_dir = '/kaggle/input/deep-learning-mini-project-spring-24-nyu/cifar-10-python/cifar-10-batches-py'\n",
    "\n",
    "# Load the label names\n",
    "meta_data_dict = load_cifar_batch1('cifar-10-batches-py/batches.meta')\n",
    "label_names = meta_data_dict[b'label_names']\n",
    "\n",
    "\n",
    "def load_cifar_batch(file, transform=None):\n",
    "    with open(file, 'rb') as fo:\n",
    "        batch = pickle.load(fo, encoding='bytes')\n",
    "    images = batch[b'data']\n",
    "    if len(batch) > 2:\n",
    "        labels = batch[b'labels']\n",
    "    num_images = images.shape[0]\n",
    "    \n",
    "    # Reshape the images into the correct format: N x H x W x C\n",
    "    # print(images)\n",
    "    # images = images.reshape((num_images, 3, 32, 32)).transpose(0, 2, 3, 1).astype(np.float32)\n",
    "    \n",
    "    # for print out\n",
    "    images = images.reshape((-1, 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "\n",
    "    # plt.figure(figsize=(20, 4))\n",
    "    # for i in range(10):\n",
    "    #     plt.subplot(1, 10, i+1)\n",
    "    #     plt.imshow(images[i])\n",
    "    #     plt.title(label_names[labels[i]].decode('utf-8'))  # Decoding from bytes to string\n",
    "    #     plt.axis('off')\n",
    "    # plt.show()\n",
    "    \n",
    "    min_val = np.min(images)\n",
    "    max_val = np.max(images)\n",
    "    images = (images - min_val) / (max_val - min_val) #normalizing facepalm\n",
    "\n",
    "    # print(images)\n",
    "    # images = images.reshape((num_images, 3, 32, 32)).astype(np.float32)\n",
    "    # Initialize a list to store processed images\n",
    "    processed_images = []\n",
    "\n",
    "\n",
    "    # Apply transformations if any\n",
    "    for i,image in enumerate(images):\n",
    "        if transform is not None:\n",
    "            # Convert numpy array to PIL Image to apply transformation\n",
    "            pil_image = Image.fromarray((image * 255).astype(np.uint8))\n",
    "            # Apply transformation\n",
    "            pil_image = transform(pil_image)\n",
    "       \n",
    "            tup = (pil_image, labels[i])\n",
    "            # Append transformed image\n",
    "            processed_images.append(tup)\n",
    "        else:\n",
    "            # Convert to tensor without transformation\n",
    "            torch.tensor(image, dtype=torch.float32) / 255.0\n",
    "            processed_images.append(image, labels[i])\n",
    "    # print(processed_images[:1])\n",
    "    return processed_images\n",
    "\n",
    "\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "# transformation for train data \n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(10), # Add random rotation with 10 degrees\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1), # Add color jitter\n",
    "    transforms.RandomHorizontalFlip(), #apply horizontal flipping\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    # transforms.RandomHorizontalFlip(), # Randomly flip the images on the horizontal axis\n",
    "    transforms.RandomRotation(10), # Randomly rotate the images by +/- 10 degrees\n",
    "    # transforms.RandomCrop(32, padding=4), # Apply random crops\n",
    "    # transforms.RandomCrop(32, padding=4),\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    # transforms.RandomRotation(10),\n",
    "    # transforms.RandomHorizontalFlip(0.5),\n",
    "    # transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "batch_files = ['cifar-10-batches-py/data_batch_1', 'cifar-10-batches-py/data_batch_2', 'cifar-10-batches-py/data_batch_3', 'cifar-10-batches-py/data_batch_4', 'cifar-10-batches-py/data_batch_5']\n",
    "trainarr = []\n",
    "trainlabel = None\n",
    "\n",
    "for file in batch_files:\n",
    "    # Load CIFAR-10 batch\n",
    "    batch = load_cifar_batch(file, transform=train_transform)\n",
    "    for item in batch:\n",
    "        trainarr.append(item)\n",
    "    # print(type(trainarr))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# print(type(trainarr))\n",
    "batch_size = 16\n",
    "trainDataLoader = torch.utils.data.DataLoader(trainarr, batch_size=batch_size,shuffle=True)\n",
    "# trainDataLoaderLabels = torch.utils.data.DataLoader(trainlabel, batch_size=batch_size,shuffle=True)\n",
    "# print(next(iter(trainDataLoader)))\n",
    "# print(len(trainarr))\n",
    "# print(trainarr[40000:40030])\n",
    "\n",
    "\n",
    "test_images = load_cifar_batch('cifar-10-batches-py/test_batch', transform=test_transform)\n",
    "testDataLoader = torch.utils.data.DataLoader(test_images, batch_size=batch_size,shuffle=True)\n",
    "# testDataLoaderLabels = torch.utils.data.DataLoader(test_labels, batch_size=batch_size,shuffle=True)\n",
    "\n",
    "# images = next(iter(trainDataLoader))\n",
    "# labels = next(iter(trainDataLoaderLabels))\n",
    "print(len(trainDataLoader))\n",
    "# for batch in trainDataLoader:\n",
    "#     print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bf40242-236c-4e92-a188-f141bb24e04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "# from torch.optim.lr_scheduler import StepLR \n",
    "\n",
    "\n",
    "input_size = 28 # each input token is a row of a FashionMNIST image so 28 pixels\n",
    "hidden_size=128 # hidden representation size\n",
    "num_layers = 2  # two-layer LSTM\n",
    "num_classes = 10\n",
    "num_epochs=18\n",
    "learning_rate= 0.01\n",
    "\n",
    "\n",
    "model = ResNet18().cuda()\n",
    "loss = torch.nn.CrossEntropyLoss() # Step 2: loss\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=.001)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "\n",
    "# overfitting\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)\n",
    "# scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.003, momentum=0.9, weight_decay=1e-5)\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=18)  # T_max is typically set to the number of epochs\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e2ebdc1-9552-4e97-8362-cc871798e82a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:  11173962\n",
      "Epoch [1/18], Step [100/3125], Loss: 2.19\n",
      "Epoch [1/18], Step [200/3125], Loss: 2.41\n",
      "Epoch [1/18], Step [300/3125], Loss: 1.83\n",
      "Epoch [1/18], Step [400/3125], Loss: 2.03\n",
      "Epoch [1/18], Step [500/3125], Loss: 1.82\n",
      "Epoch [1/18], Step [600/3125], Loss: 1.76\n",
      "Epoch [1/18], Step [700/3125], Loss: 1.58\n",
      "Epoch [1/18], Step [800/3125], Loss: 2.19\n",
      "Epoch [1/18], Step [900/3125], Loss: 1.09\n",
      "Epoch [1/18], Step [1000/3125], Loss: 1.51\n",
      "Epoch [1/18], Step [1100/3125], Loss: 1.38\n",
      "Epoch [1/18], Step [1200/3125], Loss: 1.98\n",
      "Epoch [1/18], Step [1300/3125], Loss: 1.77\n",
      "Epoch [1/18], Step [1400/3125], Loss: 1.18\n",
      "Epoch [1/18], Step [1500/3125], Loss: 1.73\n",
      "Epoch [1/18], Step [1600/3125], Loss: 1.80\n",
      "Epoch [1/18], Step [1700/3125], Loss: 0.91\n",
      "Epoch [1/18], Step [1800/3125], Loss: 1.65\n",
      "Epoch [1/18], Step [1900/3125], Loss: 1.25\n",
      "Epoch [1/18], Step [2000/3125], Loss: 1.46\n",
      "Epoch [1/18], Step [2100/3125], Loss: 0.88\n",
      "Epoch [1/18], Step [2200/3125], Loss: 1.68\n",
      "Epoch [1/18], Step [2300/3125], Loss: 1.23\n",
      "Epoch [1/18], Step [2400/3125], Loss: 1.15\n",
      "Epoch [1/18], Step [2500/3125], Loss: 1.04\n",
      "Epoch [1/18], Step [2600/3125], Loss: 2.16\n",
      "Epoch [1/18], Step [2700/3125], Loss: 1.59\n",
      "Epoch [1/18], Step [2800/3125], Loss: 1.12\n",
      "Epoch [1/18], Step [2900/3125], Loss: 1.00\n",
      "Epoch [1/18], Step [3000/3125], Loss: 1.11\n",
      "Epoch [1/18], Step [3100/3125], Loss: 1.61\n",
      "Epoch [2/18], Step [100/3125], Loss: 2.06\n",
      "Epoch [2/18], Step [200/3125], Loss: 0.91\n",
      "Epoch [2/18], Step [300/3125], Loss: 0.87\n",
      "Epoch [2/18], Step [400/3125], Loss: 1.09\n",
      "Epoch [2/18], Step [500/3125], Loss: 1.54\n",
      "Epoch [2/18], Step [600/3125], Loss: 0.89\n",
      "Epoch [2/18], Step [700/3125], Loss: 1.19\n",
      "Epoch [2/18], Step [800/3125], Loss: 1.09\n",
      "Epoch [2/18], Step [900/3125], Loss: 1.40\n",
      "Epoch [2/18], Step [1000/3125], Loss: 0.90\n",
      "Epoch [2/18], Step [1100/3125], Loss: 0.86\n",
      "Epoch [2/18], Step [1200/3125], Loss: 0.90\n",
      "Epoch [2/18], Step [1300/3125], Loss: 0.65\n",
      "Epoch [2/18], Step [1400/3125], Loss: 0.66\n",
      "Epoch [2/18], Step [1500/3125], Loss: 0.88\n",
      "Epoch [2/18], Step [1600/3125], Loss: 1.07\n",
      "Epoch [2/18], Step [1700/3125], Loss: 0.88\n",
      "Epoch [2/18], Step [1800/3125], Loss: 1.39\n",
      "Epoch [2/18], Step [1900/3125], Loss: 0.78\n",
      "Epoch [2/18], Step [2000/3125], Loss: 0.75\n",
      "Epoch [2/18], Step [2100/3125], Loss: 0.82\n",
      "Epoch [2/18], Step [2200/3125], Loss: 1.30\n",
      "Epoch [2/18], Step [2300/3125], Loss: 1.29\n",
      "Epoch [2/18], Step [2400/3125], Loss: 0.61\n",
      "Epoch [2/18], Step [2500/3125], Loss: 0.96\n",
      "Epoch [2/18], Step [2600/3125], Loss: 1.03\n",
      "Epoch [2/18], Step [2700/3125], Loss: 0.98\n",
      "Epoch [2/18], Step [2800/3125], Loss: 0.97\n",
      "Epoch [2/18], Step [2900/3125], Loss: 0.41\n",
      "Epoch [2/18], Step [3000/3125], Loss: 1.02\n",
      "Epoch [2/18], Step [3100/3125], Loss: 1.25\n",
      "Epoch [3/18], Step [100/3125], Loss: 0.92\n",
      "Epoch [3/18], Step [200/3125], Loss: 1.17\n",
      "Epoch [3/18], Step [300/3125], Loss: 1.07\n",
      "Epoch [3/18], Step [400/3125], Loss: 0.90\n",
      "Epoch [3/18], Step [500/3125], Loss: 1.26\n",
      "Epoch [3/18], Step [600/3125], Loss: 0.82\n",
      "Epoch [3/18], Step [700/3125], Loss: 1.17\n",
      "Epoch [3/18], Step [800/3125], Loss: 0.69\n",
      "Epoch [3/18], Step [900/3125], Loss: 0.77\n",
      "Epoch [3/18], Step [1000/3125], Loss: 1.22\n",
      "Epoch [3/18], Step [1100/3125], Loss: 1.47\n",
      "Epoch [3/18], Step [1200/3125], Loss: 0.60\n",
      "Epoch [3/18], Step [1300/3125], Loss: 0.41\n",
      "Epoch [3/18], Step [1400/3125], Loss: 0.73\n",
      "Epoch [3/18], Step [1500/3125], Loss: 0.76\n",
      "Epoch [3/18], Step [1600/3125], Loss: 0.42\n",
      "Epoch [3/18], Step [1700/3125], Loss: 1.67\n",
      "Epoch [3/18], Step [1800/3125], Loss: 0.70\n",
      "Epoch [3/18], Step [1900/3125], Loss: 1.24\n",
      "Epoch [3/18], Step [2000/3125], Loss: 0.53\n",
      "Epoch [3/18], Step [2100/3125], Loss: 0.77\n",
      "Epoch [3/18], Step [2200/3125], Loss: 0.62\n",
      "Epoch [3/18], Step [2300/3125], Loss: 0.39\n",
      "Epoch [3/18], Step [2400/3125], Loss: 1.39\n",
      "Epoch [3/18], Step [2500/3125], Loss: 0.87\n",
      "Epoch [3/18], Step [2600/3125], Loss: 0.67\n",
      "Epoch [3/18], Step [2700/3125], Loss: 0.67\n",
      "Epoch [3/18], Step [2800/3125], Loss: 1.00\n",
      "Epoch [3/18], Step [2900/3125], Loss: 1.49\n",
      "Epoch [3/18], Step [3000/3125], Loss: 0.87\n",
      "Epoch [3/18], Step [3100/3125], Loss: 0.82\n",
      "Epoch [4/18], Step [100/3125], Loss: 0.55\n",
      "Epoch [4/18], Step [200/3125], Loss: 1.29\n",
      "Epoch [4/18], Step [300/3125], Loss: 0.58\n",
      "Epoch [4/18], Step [400/3125], Loss: 0.24\n",
      "Epoch [4/18], Step [500/3125], Loss: 0.99\n",
      "Epoch [4/18], Step [600/3125], Loss: 1.23\n",
      "Epoch [4/18], Step [700/3125], Loss: 1.55\n",
      "Epoch [4/18], Step [800/3125], Loss: 0.33\n",
      "Epoch [4/18], Step [900/3125], Loss: 0.61\n",
      "Epoch [4/18], Step [1000/3125], Loss: 0.82\n",
      "Epoch [4/18], Step [1100/3125], Loss: 0.92\n",
      "Epoch [4/18], Step [1200/3125], Loss: 1.59\n",
      "Epoch [4/18], Step [1300/3125], Loss: 0.38\n",
      "Epoch [4/18], Step [1400/3125], Loss: 0.54\n",
      "Epoch [4/18], Step [1500/3125], Loss: 0.32\n",
      "Epoch [4/18], Step [1600/3125], Loss: 0.69\n",
      "Epoch [4/18], Step [1700/3125], Loss: 0.43\n",
      "Epoch [4/18], Step [1800/3125], Loss: 0.30\n",
      "Epoch [4/18], Step [1900/3125], Loss: 1.09\n",
      "Epoch [4/18], Step [2000/3125], Loss: 0.94\n",
      "Epoch [4/18], Step [2100/3125], Loss: 1.08\n",
      "Epoch [4/18], Step [2200/3125], Loss: 0.37\n",
      "Epoch [4/18], Step [2300/3125], Loss: 0.71\n",
      "Epoch [4/18], Step [2400/3125], Loss: 0.59\n",
      "Epoch [4/18], Step [2500/3125], Loss: 0.73\n",
      "Epoch [4/18], Step [2600/3125], Loss: 0.78\n",
      "Epoch [4/18], Step [2700/3125], Loss: 0.87\n",
      "Epoch [4/18], Step [2800/3125], Loss: 0.86\n",
      "Epoch [4/18], Step [2900/3125], Loss: 0.78\n",
      "Epoch [4/18], Step [3000/3125], Loss: 0.46\n",
      "Epoch [4/18], Step [3100/3125], Loss: 0.84\n",
      "Epoch [5/18], Step [100/3125], Loss: 0.39\n",
      "Epoch [5/18], Step [200/3125], Loss: 0.35\n",
      "Epoch [5/18], Step [300/3125], Loss: 0.45\n",
      "Epoch [5/18], Step [400/3125], Loss: 0.36\n",
      "Epoch [5/18], Step [500/3125], Loss: 0.17\n",
      "Epoch [5/18], Step [600/3125], Loss: 1.12\n",
      "Epoch [5/18], Step [700/3125], Loss: 0.37\n",
      "Epoch [5/18], Step [800/3125], Loss: 0.46\n",
      "Epoch [5/18], Step [900/3125], Loss: 0.81\n",
      "Epoch [5/18], Step [1000/3125], Loss: 0.41\n",
      "Epoch [5/18], Step [1100/3125], Loss: 1.12\n",
      "Epoch [5/18], Step [1200/3125], Loss: 0.38\n",
      "Epoch [5/18], Step [1300/3125], Loss: 0.32\n",
      "Epoch [5/18], Step [1400/3125], Loss: 0.20\n",
      "Epoch [5/18], Step [1500/3125], Loss: 0.61\n",
      "Epoch [5/18], Step [1600/3125], Loss: 0.38\n",
      "Epoch [5/18], Step [1700/3125], Loss: 0.54\n",
      "Epoch [5/18], Step [1800/3125], Loss: 0.51\n",
      "Epoch [5/18], Step [1900/3125], Loss: 0.72\n",
      "Epoch [5/18], Step [2000/3125], Loss: 0.53\n",
      "Epoch [5/18], Step [2100/3125], Loss: 0.65\n",
      "Epoch [5/18], Step [2200/3125], Loss: 0.60\n",
      "Epoch [5/18], Step [2300/3125], Loss: 0.50\n",
      "Epoch [5/18], Step [2400/3125], Loss: 0.75\n",
      "Epoch [5/18], Step [2500/3125], Loss: 0.26\n",
      "Epoch [5/18], Step [2600/3125], Loss: 0.50\n",
      "Epoch [5/18], Step [2700/3125], Loss: 0.24\n",
      "Epoch [5/18], Step [2800/3125], Loss: 0.47\n",
      "Epoch [5/18], Step [2900/3125], Loss: 0.39\n",
      "Epoch [5/18], Step [3000/3125], Loss: 0.91\n",
      "Epoch [5/18], Step [3100/3125], Loss: 0.91\n",
      "Epoch [6/18], Step [100/3125], Loss: 0.09\n",
      "Epoch [6/18], Step [200/3125], Loss: 0.32\n",
      "Epoch [6/18], Step [300/3125], Loss: 0.21\n",
      "Epoch [6/18], Step [400/3125], Loss: 0.54\n",
      "Epoch [6/18], Step [500/3125], Loss: 0.33\n",
      "Epoch [6/18], Step [600/3125], Loss: 0.50\n",
      "Epoch [6/18], Step [700/3125], Loss: 0.85\n",
      "Epoch [6/18], Step [800/3125], Loss: 0.13\n",
      "Epoch [6/18], Step [900/3125], Loss: 0.39\n",
      "Epoch [6/18], Step [1000/3125], Loss: 0.32\n",
      "Epoch [6/18], Step [1100/3125], Loss: 0.47\n",
      "Epoch [6/18], Step [1200/3125], Loss: 0.36\n",
      "Epoch [6/18], Step [1300/3125], Loss: 0.64\n",
      "Epoch [6/18], Step [1400/3125], Loss: 0.80\n",
      "Epoch [6/18], Step [1500/3125], Loss: 0.36\n",
      "Epoch [6/18], Step [1600/3125], Loss: 0.40\n",
      "Epoch [6/18], Step [1700/3125], Loss: 0.54\n",
      "Epoch [6/18], Step [1800/3125], Loss: 0.49\n",
      "Epoch [6/18], Step [1900/3125], Loss: 0.64\n",
      "Epoch [6/18], Step [2000/3125], Loss: 0.17\n",
      "Epoch [6/18], Step [2100/3125], Loss: 0.35\n",
      "Epoch [6/18], Step [2200/3125], Loss: 0.87\n",
      "Epoch [6/18], Step [2300/3125], Loss: 0.09\n",
      "Epoch [6/18], Step [2400/3125], Loss: 0.37\n",
      "Epoch [6/18], Step [2500/3125], Loss: 0.35\n",
      "Epoch [6/18], Step [2600/3125], Loss: 0.50\n",
      "Epoch [6/18], Step [2700/3125], Loss: 0.67\n",
      "Epoch [6/18], Step [2800/3125], Loss: 0.38\n",
      "Epoch [6/18], Step [2900/3125], Loss: 0.46\n",
      "Epoch [6/18], Step [3000/3125], Loss: 0.32\n",
      "Epoch [6/18], Step [3100/3125], Loss: 0.31\n",
      "Epoch [7/18], Step [100/3125], Loss: 0.28\n",
      "Epoch [7/18], Step [200/3125], Loss: 0.32\n",
      "Epoch [7/18], Step [300/3125], Loss: 0.17\n",
      "Epoch [7/18], Step [400/3125], Loss: 0.36\n",
      "Epoch [7/18], Step [500/3125], Loss: 0.08\n",
      "Epoch [7/18], Step [600/3125], Loss: 0.38\n",
      "Epoch [7/18], Step [700/3125], Loss: 0.22\n",
      "Epoch [7/18], Step [800/3125], Loss: 0.21\n",
      "Epoch [7/18], Step [900/3125], Loss: 0.09\n",
      "Epoch [7/18], Step [1000/3125], Loss: 0.34\n",
      "Epoch [7/18], Step [1100/3125], Loss: 0.44\n",
      "Epoch [7/18], Step [1200/3125], Loss: 0.21\n",
      "Epoch [7/18], Step [1300/3125], Loss: 0.18\n",
      "Epoch [7/18], Step [1400/3125], Loss: 0.49\n",
      "Epoch [7/18], Step [1500/3125], Loss: 0.45\n",
      "Epoch [7/18], Step [1600/3125], Loss: 0.20\n",
      "Epoch [7/18], Step [1700/3125], Loss: 0.13\n",
      "Epoch [7/18], Step [1800/3125], Loss: 0.24\n",
      "Epoch [7/18], Step [1900/3125], Loss: 0.65\n",
      "Epoch [7/18], Step [2000/3125], Loss: 0.08\n",
      "Epoch [7/18], Step [2100/3125], Loss: 0.52\n",
      "Epoch [7/18], Step [2200/3125], Loss: 0.27\n",
      "Epoch [7/18], Step [2300/3125], Loss: 0.17\n",
      "Epoch [7/18], Step [2400/3125], Loss: 0.25\n",
      "Epoch [7/18], Step [2500/3125], Loss: 0.19\n",
      "Epoch [7/18], Step [2600/3125], Loss: 0.23\n",
      "Epoch [7/18], Step [2700/3125], Loss: 0.52\n",
      "Epoch [7/18], Step [2800/3125], Loss: 0.13\n",
      "Epoch [7/18], Step [2900/3125], Loss: 0.48\n",
      "Epoch [7/18], Step [3000/3125], Loss: 0.28\n",
      "Epoch [7/18], Step [3100/3125], Loss: 0.05\n",
      "Epoch [8/18], Step [100/3125], Loss: 0.16\n",
      "Epoch [8/18], Step [200/3125], Loss: 0.08\n",
      "Epoch [8/18], Step [300/3125], Loss: 0.03\n",
      "Epoch [8/18], Step [400/3125], Loss: 0.13\n",
      "Epoch [8/18], Step [500/3125], Loss: 0.10\n",
      "Epoch [8/18], Step [600/3125], Loss: 0.06\n",
      "Epoch [8/18], Step [700/3125], Loss: 0.18\n",
      "Epoch [8/18], Step [800/3125], Loss: 0.15\n",
      "Epoch [8/18], Step [900/3125], Loss: 0.29\n",
      "Epoch [8/18], Step [1000/3125], Loss: 0.24\n",
      "Epoch [8/18], Step [1100/3125], Loss: 0.22\n",
      "Epoch [8/18], Step [1200/3125], Loss: 0.22\n",
      "Epoch [8/18], Step [1300/3125], Loss: 0.32\n",
      "Epoch [8/18], Step [1400/3125], Loss: 0.04\n",
      "Epoch [8/18], Step [1500/3125], Loss: 0.56\n",
      "Epoch [8/18], Step [1600/3125], Loss: 0.28\n",
      "Epoch [8/18], Step [1700/3125], Loss: 0.12\n",
      "Epoch [8/18], Step [1800/3125], Loss: 0.23\n",
      "Epoch [8/18], Step [1900/3125], Loss: 0.12\n",
      "Epoch [8/18], Step [2000/3125], Loss: 0.33\n",
      "Epoch [8/18], Step [2100/3125], Loss: 0.13\n",
      "Epoch [8/18], Step [2200/3125], Loss: 0.44\n",
      "Epoch [8/18], Step [2300/3125], Loss: 0.06\n",
      "Epoch [8/18], Step [2400/3125], Loss: 0.17\n",
      "Epoch [8/18], Step [2500/3125], Loss: 0.45\n",
      "Epoch [8/18], Step [2600/3125], Loss: 0.44\n",
      "Epoch [8/18], Step [2700/3125], Loss: 0.11\n",
      "Epoch [8/18], Step [2800/3125], Loss: 0.20\n",
      "Epoch [8/18], Step [2900/3125], Loss: 0.13\n",
      "Epoch [8/18], Step [3000/3125], Loss: 0.06\n",
      "Epoch [8/18], Step [3100/3125], Loss: 0.24\n",
      "Epoch [9/18], Step [100/3125], Loss: 0.21\n",
      "Epoch [9/18], Step [200/3125], Loss: 0.19\n",
      "Epoch [9/18], Step [300/3125], Loss: 0.09\n",
      "Epoch [9/18], Step [400/3125], Loss: 0.44\n",
      "Epoch [9/18], Step [500/3125], Loss: 0.24\n",
      "Epoch [9/18], Step [600/3125], Loss: 0.27\n",
      "Epoch [9/18], Step [700/3125], Loss: 0.12\n",
      "Epoch [9/18], Step [800/3125], Loss: 0.07\n",
      "Epoch [9/18], Step [900/3125], Loss: 0.11\n",
      "Epoch [9/18], Step [1000/3125], Loss: 0.02\n",
      "Epoch [9/18], Step [1100/3125], Loss: 0.01\n",
      "Epoch [9/18], Step [1200/3125], Loss: 0.22\n",
      "Epoch [9/18], Step [1300/3125], Loss: 0.21\n",
      "Epoch [9/18], Step [1400/3125], Loss: 0.25\n",
      "Epoch [9/18], Step [1500/3125], Loss: 0.08\n",
      "Epoch [9/18], Step [1600/3125], Loss: 0.00\n",
      "Epoch [9/18], Step [1700/3125], Loss: 0.19\n",
      "Epoch [9/18], Step [1800/3125], Loss: 0.07\n",
      "Epoch [9/18], Step [1900/3125], Loss: 0.26\n",
      "Epoch [9/18], Step [2000/3125], Loss: 0.21\n",
      "Epoch [9/18], Step [2100/3125], Loss: 0.09\n",
      "Epoch [9/18], Step [2200/3125], Loss: 0.07\n",
      "Epoch [9/18], Step [2300/3125], Loss: 0.35\n",
      "Epoch [9/18], Step [2400/3125], Loss: 0.12\n",
      "Epoch [9/18], Step [2500/3125], Loss: 0.16\n",
      "Epoch [9/18], Step [2600/3125], Loss: 0.06\n",
      "Epoch [9/18], Step [2700/3125], Loss: 0.10\n",
      "Epoch [9/18], Step [2800/3125], Loss: 0.21\n",
      "Epoch [9/18], Step [2900/3125], Loss: 0.29\n",
      "Epoch [9/18], Step [3000/3125], Loss: 0.32\n",
      "Epoch [9/18], Step [3100/3125], Loss: 0.19\n",
      "Epoch [10/18], Step [100/3125], Loss: 0.10\n",
      "Epoch [10/18], Step [200/3125], Loss: 0.02\n",
      "Epoch [10/18], Step [300/3125], Loss: 0.14\n",
      "Epoch [10/18], Step [400/3125], Loss: 0.43\n",
      "Epoch [10/18], Step [500/3125], Loss: 0.08\n",
      "Epoch [10/18], Step [600/3125], Loss: 0.16\n",
      "Epoch [10/18], Step [700/3125], Loss: 0.02\n",
      "Epoch [10/18], Step [800/3125], Loss: 0.16\n",
      "Epoch [10/18], Step [900/3125], Loss: 0.01\n",
      "Epoch [10/18], Step [1000/3125], Loss: 0.06\n",
      "Epoch [10/18], Step [1100/3125], Loss: 0.13\n",
      "Epoch [10/18], Step [1200/3125], Loss: 0.02\n",
      "Epoch [10/18], Step [1300/3125], Loss: 0.07\n",
      "Epoch [10/18], Step [1400/3125], Loss: 0.03\n",
      "Epoch [10/18], Step [1500/3125], Loss: 0.39\n",
      "Epoch [10/18], Step [1600/3125], Loss: 0.51\n",
      "Epoch [10/18], Step [1700/3125], Loss: 0.02\n",
      "Epoch [10/18], Step [1800/3125], Loss: 0.05\n",
      "Epoch [10/18], Step [1900/3125], Loss: 0.10\n",
      "Epoch [10/18], Step [2000/3125], Loss: 0.03\n",
      "Epoch [10/18], Step [2100/3125], Loss: 0.02\n",
      "Epoch [10/18], Step [2200/3125], Loss: 0.07\n",
      "Epoch [10/18], Step [2300/3125], Loss: 0.08\n",
      "Epoch [10/18], Step [2400/3125], Loss: 0.14\n",
      "Epoch [10/18], Step [2500/3125], Loss: 0.15\n",
      "Epoch [10/18], Step [2600/3125], Loss: 0.04\n",
      "Epoch [10/18], Step [2700/3125], Loss: 0.03\n",
      "Epoch [10/18], Step [2800/3125], Loss: 0.10\n",
      "Epoch [10/18], Step [2900/3125], Loss: 0.62\n",
      "Epoch [10/18], Step [3000/3125], Loss: 0.68\n",
      "Epoch [10/18], Step [3100/3125], Loss: 0.05\n",
      "Epoch [11/18], Step [100/3125], Loss: 0.04\n",
      "Epoch [11/18], Step [200/3125], Loss: 0.09\n",
      "Epoch [11/18], Step [300/3125], Loss: 0.03\n",
      "Epoch [11/18], Step [400/3125], Loss: 0.08\n",
      "Epoch [11/18], Step [500/3125], Loss: 0.53\n",
      "Epoch [11/18], Step [600/3125], Loss: 0.06\n",
      "Epoch [11/18], Step [700/3125], Loss: 0.15\n",
      "Epoch [11/18], Step [800/3125], Loss: 0.00\n",
      "Epoch [11/18], Step [900/3125], Loss: 0.10\n",
      "Epoch [11/18], Step [1000/3125], Loss: 0.21\n",
      "Epoch [11/18], Step [1100/3125], Loss: 0.10\n",
      "Epoch [11/18], Step [1200/3125], Loss: 0.14\n",
      "Epoch [11/18], Step [1300/3125], Loss: 0.23\n",
      "Epoch [11/18], Step [1400/3125], Loss: 0.02\n",
      "Epoch [11/18], Step [1500/3125], Loss: 0.06\n",
      "Epoch [11/18], Step [1600/3125], Loss: 0.40\n",
      "Epoch [11/18], Step [1700/3125], Loss: 0.01\n",
      "Epoch [11/18], Step [1800/3125], Loss: 0.01\n",
      "Epoch [11/18], Step [1900/3125], Loss: 0.02\n",
      "Epoch [11/18], Step [2000/3125], Loss: 0.08\n",
      "Epoch [11/18], Step [2100/3125], Loss: 0.13\n",
      "Epoch [11/18], Step [2200/3125], Loss: 0.07\n",
      "Epoch [11/18], Step [2300/3125], Loss: 0.15\n",
      "Epoch [11/18], Step [2400/3125], Loss: 0.17\n",
      "Epoch [11/18], Step [2500/3125], Loss: 0.61\n",
      "Epoch [11/18], Step [2600/3125], Loss: 0.12\n",
      "Epoch [11/18], Step [2700/3125], Loss: 0.02\n",
      "Epoch [11/18], Step [2800/3125], Loss: 0.11\n",
      "Epoch [11/18], Step [2900/3125], Loss: 0.06\n",
      "Epoch [11/18], Step [3000/3125], Loss: 0.03\n",
      "Epoch [11/18], Step [3100/3125], Loss: 0.41\n",
      "Epoch [12/18], Step [100/3125], Loss: 0.03\n",
      "Epoch [12/18], Step [200/3125], Loss: 0.04\n",
      "Epoch [12/18], Step [300/3125], Loss: 0.02\n",
      "Epoch [12/18], Step [400/3125], Loss: 0.03\n",
      "Epoch [12/18], Step [500/3125], Loss: 0.06\n",
      "Epoch [12/18], Step [600/3125], Loss: 0.17\n",
      "Epoch [12/18], Step [700/3125], Loss: 0.03\n",
      "Epoch [12/18], Step [800/3125], Loss: 0.10\n",
      "Epoch [12/18], Step [900/3125], Loss: 0.00\n",
      "Epoch [12/18], Step [1000/3125], Loss: 0.06\n",
      "Epoch [12/18], Step [1100/3125], Loss: 0.04\n",
      "Epoch [12/18], Step [1200/3125], Loss: 0.11\n",
      "Epoch [12/18], Step [1300/3125], Loss: 0.02\n",
      "Epoch [12/18], Step [1400/3125], Loss: 0.09\n",
      "Epoch [12/18], Step [1500/3125], Loss: 0.02\n",
      "Epoch [12/18], Step [1600/3125], Loss: 0.08\n",
      "Epoch [12/18], Step [1700/3125], Loss: 0.29\n",
      "Epoch [12/18], Step [1800/3125], Loss: 0.02\n",
      "Epoch [12/18], Step [1900/3125], Loss: 0.08\n",
      "Epoch [12/18], Step [2000/3125], Loss: 0.08\n",
      "Epoch [12/18], Step [2100/3125], Loss: 0.01\n",
      "Epoch [12/18], Step [2200/3125], Loss: 0.09\n",
      "Epoch [12/18], Step [2300/3125], Loss: 0.21\n",
      "Epoch [12/18], Step [2400/3125], Loss: 0.04\n",
      "Epoch [12/18], Step [2500/3125], Loss: 0.03\n",
      "Epoch [12/18], Step [2600/3125], Loss: 0.10\n",
      "Epoch [12/18], Step [2700/3125], Loss: 0.12\n",
      "Epoch [12/18], Step [2800/3125], Loss: 0.04\n",
      "Epoch [12/18], Step [2900/3125], Loss: 0.04\n",
      "Epoch [12/18], Step [3000/3125], Loss: 0.06\n",
      "Epoch [12/18], Step [3100/3125], Loss: 0.05\n",
      "Epoch [13/18], Step [100/3125], Loss: 0.01\n",
      "Epoch [13/18], Step [200/3125], Loss: 0.02\n",
      "Epoch [13/18], Step [300/3125], Loss: 0.03\n",
      "Epoch [13/18], Step [400/3125], Loss: 0.05\n",
      "Epoch [13/18], Step [500/3125], Loss: 0.03\n",
      "Epoch [13/18], Step [600/3125], Loss: 0.09\n",
      "Epoch [13/18], Step [700/3125], Loss: 0.01\n",
      "Epoch [13/18], Step [800/3125], Loss: 0.02\n",
      "Epoch [13/18], Step [900/3125], Loss: 0.04\n",
      "Epoch [13/18], Step [1000/3125], Loss: 0.00\n",
      "Epoch [13/18], Step [1100/3125], Loss: 0.16\n",
      "Epoch [13/18], Step [1200/3125], Loss: 0.11\n",
      "Epoch [13/18], Step [1300/3125], Loss: 0.34\n",
      "Epoch [13/18], Step [1400/3125], Loss: 0.02\n",
      "Epoch [13/18], Step [1500/3125], Loss: 0.02\n",
      "Epoch [13/18], Step [1600/3125], Loss: 0.05\n",
      "Epoch [13/18], Step [1700/3125], Loss: 0.27\n",
      "Epoch [13/18], Step [1800/3125], Loss: 0.01\n",
      "Epoch [13/18], Step [1900/3125], Loss: 0.03\n",
      "Epoch [13/18], Step [2000/3125], Loss: 0.00\n",
      "Epoch [13/18], Step [2100/3125], Loss: 0.01\n",
      "Epoch [13/18], Step [2200/3125], Loss: 0.08\n",
      "Epoch [13/18], Step [2300/3125], Loss: 0.03\n",
      "Epoch [13/18], Step [2400/3125], Loss: 0.01\n",
      "Epoch [13/18], Step [2500/3125], Loss: 0.04\n",
      "Epoch [13/18], Step [2600/3125], Loss: 0.05\n",
      "Epoch [13/18], Step [2700/3125], Loss: 0.02\n",
      "Epoch [13/18], Step [2800/3125], Loss: 0.07\n",
      "Epoch [13/18], Step [2900/3125], Loss: 0.11\n",
      "Epoch [13/18], Step [3000/3125], Loss: 0.01\n",
      "Epoch [13/18], Step [3100/3125], Loss: 0.05\n",
      "Epoch [14/18], Step [100/3125], Loss: 0.03\n",
      "Epoch [14/18], Step [200/3125], Loss: 0.13\n",
      "Epoch [14/18], Step [300/3125], Loss: 0.05\n",
      "Epoch [14/18], Step [400/3125], Loss: 0.00\n",
      "Epoch [14/18], Step [500/3125], Loss: 0.35\n",
      "Epoch [14/18], Step [600/3125], Loss: 0.08\n",
      "Epoch [14/18], Step [700/3125], Loss: 0.05\n",
      "Epoch [14/18], Step [800/3125], Loss: 0.03\n",
      "Epoch [14/18], Step [900/3125], Loss: 0.01\n",
      "Epoch [14/18], Step [1000/3125], Loss: 0.04\n",
      "Epoch [14/18], Step [1100/3125], Loss: 0.01\n",
      "Epoch [14/18], Step [1200/3125], Loss: 0.01\n",
      "Epoch [14/18], Step [1300/3125], Loss: 0.01\n",
      "Epoch [14/18], Step [1400/3125], Loss: 0.03\n",
      "Epoch [14/18], Step [1500/3125], Loss: 0.09\n",
      "Epoch [14/18], Step [1600/3125], Loss: 0.31\n",
      "Epoch [14/18], Step [1700/3125], Loss: 0.13\n",
      "Epoch [14/18], Step [1800/3125], Loss: 0.15\n",
      "Epoch [14/18], Step [1900/3125], Loss: 0.09\n",
      "Epoch [14/18], Step [2000/3125], Loss: 0.02\n",
      "Epoch [14/18], Step [2100/3125], Loss: 0.02\n",
      "Epoch [14/18], Step [2200/3125], Loss: 0.01\n",
      "Epoch [14/18], Step [2300/3125], Loss: 0.03\n",
      "Epoch [14/18], Step [2400/3125], Loss: 0.02\n",
      "Epoch [14/18], Step [2500/3125], Loss: 0.03\n",
      "Epoch [14/18], Step [2600/3125], Loss: 0.08\n",
      "Epoch [14/18], Step [2700/3125], Loss: 0.00\n",
      "Epoch [14/18], Step [2800/3125], Loss: 0.00\n",
      "Epoch [14/18], Step [2900/3125], Loss: 0.04\n",
      "Epoch [14/18], Step [3000/3125], Loss: 0.02\n",
      "Epoch [14/18], Step [3100/3125], Loss: 0.01\n",
      "Epoch [15/18], Step [100/3125], Loss: 0.01\n",
      "Epoch [15/18], Step [200/3125], Loss: 0.01\n",
      "Epoch [15/18], Step [300/3125], Loss: 0.16\n",
      "Epoch [15/18], Step [400/3125], Loss: 0.03\n",
      "Epoch [15/18], Step [500/3125], Loss: 0.03\n",
      "Epoch [15/18], Step [600/3125], Loss: 0.01\n",
      "Epoch [15/18], Step [700/3125], Loss: 0.00\n",
      "Epoch [15/18], Step [800/3125], Loss: 0.26\n",
      "Epoch [15/18], Step [900/3125], Loss: 0.02\n",
      "Epoch [15/18], Step [1000/3125], Loss: 0.03\n",
      "Epoch [15/18], Step [1100/3125], Loss: 0.03\n",
      "Epoch [15/18], Step [1200/3125], Loss: 0.13\n",
      "Epoch [15/18], Step [1300/3125], Loss: 0.13\n",
      "Epoch [15/18], Step [1400/3125], Loss: 0.01\n",
      "Epoch [15/18], Step [1500/3125], Loss: 0.03\n",
      "Epoch [15/18], Step [1600/3125], Loss: 0.22\n",
      "Epoch [15/18], Step [1700/3125], Loss: 0.01\n",
      "Epoch [15/18], Step [1800/3125], Loss: 0.00\n",
      "Epoch [15/18], Step [1900/3125], Loss: 0.03\n",
      "Epoch [15/18], Step [2000/3125], Loss: 0.02\n",
      "Epoch [15/18], Step [2100/3125], Loss: 0.06\n",
      "Epoch [15/18], Step [2200/3125], Loss: 0.23\n",
      "Epoch [15/18], Step [2300/3125], Loss: 0.00\n",
      "Epoch [15/18], Step [2400/3125], Loss: 0.20\n",
      "Epoch [15/18], Step [2500/3125], Loss: 0.00\n",
      "Epoch [15/18], Step [2600/3125], Loss: 0.13\n",
      "Epoch [15/18], Step [2700/3125], Loss: 0.03\n",
      "Epoch [15/18], Step [2800/3125], Loss: 0.08\n",
      "Epoch [15/18], Step [2900/3125], Loss: 0.27\n",
      "Epoch [15/18], Step [3000/3125], Loss: 0.07\n",
      "Epoch [15/18], Step [3100/3125], Loss: 0.03\n",
      "Epoch [16/18], Step [100/3125], Loss: 0.00\n",
      "Epoch [16/18], Step [200/3125], Loss: 0.01\n",
      "Epoch [16/18], Step [300/3125], Loss: 0.11\n",
      "Epoch [16/18], Step [400/3125], Loss: 0.01\n",
      "Epoch [16/18], Step [500/3125], Loss: 0.01\n",
      "Epoch [16/18], Step [600/3125], Loss: 0.00\n",
      "Epoch [16/18], Step [700/3125], Loss: 0.04\n",
      "Epoch [16/18], Step [800/3125], Loss: 0.00\n",
      "Epoch [16/18], Step [900/3125], Loss: 0.05\n",
      "Epoch [16/18], Step [1000/3125], Loss: 0.00\n",
      "Epoch [16/18], Step [1100/3125], Loss: 0.01\n",
      "Epoch [16/18], Step [1200/3125], Loss: 0.10\n",
      "Epoch [16/18], Step [1300/3125], Loss: 0.00\n",
      "Epoch [16/18], Step [1400/3125], Loss: 0.03\n",
      "Epoch [16/18], Step [1500/3125], Loss: 0.03\n",
      "Epoch [16/18], Step [1600/3125], Loss: 0.03\n",
      "Epoch [16/18], Step [1700/3125], Loss: 0.01\n",
      "Epoch [16/18], Step [1800/3125], Loss: 0.08\n",
      "Epoch [16/18], Step [1900/3125], Loss: 0.01\n",
      "Epoch [16/18], Step [2000/3125], Loss: 0.05\n",
      "Epoch [16/18], Step [2100/3125], Loss: 0.01\n",
      "Epoch [16/18], Step [2200/3125], Loss: 0.05\n",
      "Epoch [16/18], Step [2300/3125], Loss: 0.04\n",
      "Epoch [16/18], Step [2400/3125], Loss: 0.00\n",
      "Epoch [16/18], Step [2500/3125], Loss: 0.00\n",
      "Epoch [16/18], Step [2600/3125], Loss: 0.00\n",
      "Epoch [16/18], Step [2700/3125], Loss: 0.40\n",
      "Epoch [16/18], Step [2800/3125], Loss: 0.16\n",
      "Epoch [16/18], Step [2900/3125], Loss: 0.00\n",
      "Epoch [16/18], Step [3000/3125], Loss: 0.34\n",
      "Epoch [16/18], Step [3100/3125], Loss: 0.05\n",
      "Epoch [17/18], Step [100/3125], Loss: 0.01\n",
      "Epoch [17/18], Step [200/3125], Loss: 0.03\n",
      "Epoch [17/18], Step [300/3125], Loss: 0.02\n",
      "Epoch [17/18], Step [400/3125], Loss: 0.01\n",
      "Epoch [17/18], Step [500/3125], Loss: 0.03\n",
      "Epoch [17/18], Step [600/3125], Loss: 0.00\n",
      "Epoch [17/18], Step [700/3125], Loss: 0.01\n",
      "Epoch [17/18], Step [800/3125], Loss: 0.08\n",
      "Epoch [17/18], Step [900/3125], Loss: 0.00\n",
      "Epoch [17/18], Step [1000/3125], Loss: 0.01\n",
      "Epoch [17/18], Step [1100/3125], Loss: 0.01\n",
      "Epoch [17/18], Step [1200/3125], Loss: 0.10\n",
      "Epoch [17/18], Step [1300/3125], Loss: 0.00\n",
      "Epoch [17/18], Step [1400/3125], Loss: 0.02\n",
      "Epoch [17/18], Step [1500/3125], Loss: 0.01\n",
      "Epoch [17/18], Step [1600/3125], Loss: 0.02\n",
      "Epoch [17/18], Step [1700/3125], Loss: 0.00\n",
      "Epoch [17/18], Step [1800/3125], Loss: 0.00\n",
      "Epoch [17/18], Step [1900/3125], Loss: 0.01\n",
      "Epoch [17/18], Step [2000/3125], Loss: 0.02\n",
      "Epoch [17/18], Step [2100/3125], Loss: 0.01\n",
      "Epoch [17/18], Step [2200/3125], Loss: 0.01\n",
      "Epoch [17/18], Step [2300/3125], Loss: 0.00\n",
      "Epoch [17/18], Step [2400/3125], Loss: 0.11\n",
      "Epoch [17/18], Step [2500/3125], Loss: 0.00\n",
      "Epoch [17/18], Step [2600/3125], Loss: 0.00\n",
      "Epoch [17/18], Step [2700/3125], Loss: 0.01\n",
      "Epoch [17/18], Step [2800/3125], Loss: 0.02\n",
      "Epoch [17/18], Step [2900/3125], Loss: 0.01\n",
      "Epoch [17/18], Step [3000/3125], Loss: 0.00\n",
      "Epoch [17/18], Step [3100/3125], Loss: 0.00\n",
      "Epoch [18/18], Step [100/3125], Loss: 0.01\n",
      "Epoch [18/18], Step [200/3125], Loss: 0.02\n",
      "Epoch [18/18], Step [300/3125], Loss: 0.00\n",
      "Epoch [18/18], Step [400/3125], Loss: 0.00\n",
      "Epoch [18/18], Step [500/3125], Loss: 0.00\n",
      "Epoch [18/18], Step [600/3125], Loss: 0.00\n",
      "Epoch [18/18], Step [700/3125], Loss: 0.00\n",
      "Epoch [18/18], Step [800/3125], Loss: 0.00\n",
      "Epoch [18/18], Step [900/3125], Loss: 0.00\n",
      "Epoch [18/18], Step [1000/3125], Loss: 0.08\n",
      "Epoch [18/18], Step [1100/3125], Loss: 0.11\n",
      "Epoch [18/18], Step [1200/3125], Loss: 0.01\n",
      "Epoch [18/18], Step [1300/3125], Loss: 0.11\n",
      "Epoch [18/18], Step [1400/3125], Loss: 0.02\n",
      "Epoch [18/18], Step [1500/3125], Loss: 0.01\n",
      "Epoch [18/18], Step [1600/3125], Loss: 0.04\n",
      "Epoch [18/18], Step [1700/3125], Loss: 0.00\n",
      "Epoch [18/18], Step [1800/3125], Loss: 0.05\n",
      "Epoch [18/18], Step [1900/3125], Loss: 0.00\n",
      "Epoch [18/18], Step [2000/3125], Loss: 0.00\n",
      "Epoch [18/18], Step [2100/3125], Loss: 0.00\n",
      "Epoch [18/18], Step [2200/3125], Loss: 0.00\n",
      "Epoch [18/18], Step [2300/3125], Loss: 0.02\n",
      "Epoch [18/18], Step [2400/3125], Loss: 0.02\n",
      "Epoch [18/18], Step [2500/3125], Loss: 0.02\n",
      "Epoch [18/18], Step [2600/3125], Loss: 0.00\n",
      "Epoch [18/18], Step [2700/3125], Loss: 0.00\n",
      "Epoch [18/18], Step [2800/3125], Loss: 0.01\n",
      "Epoch [18/18], Step [2900/3125], Loss: 0.00\n",
      "Epoch [18/18], Step [3000/3125], Loss: 0.00\n",
      "Epoch [18/18], Step [3100/3125], Loss: 0.02\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "total_step = len(trainDataLoader)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Number of parameters: \", num_params)\n",
    "\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "matches = 0\n",
    "total = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  train_loss = 0.0\n",
    "  test_loss = 0.0\n",
    "\n",
    "  model.train()\n",
    "  for i, data in enumerate(trainDataLoader):\n",
    "    images, labels = data\n",
    "    images = images.cuda()\n",
    "    labels = labels.cuda()\n",
    "    # forward\n",
    "    predicted_output = model(images) # forward propagation\n",
    "    fit = loss(predicted_output, labels)  # calculate our measure of goodness\n",
    "\n",
    "    # backwards\n",
    "    optimizer.zero_grad() # zero out any gradient values from the previous iteration\n",
    "    fit.backward() # backpropagation\n",
    "    optimizer.step() # update the weights of our trainable parameters\n",
    "    train_loss += fit.item()\n",
    "    if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.2f}' \n",
    "                    .format(epoch + 1, num_epochs, i + 1, total_step, fit.item()))\n",
    "  # model.eval()\n",
    "  # for i, data in enumerate(testDataLoader):\n",
    "  #   with torch.no_grad():\n",
    "  #     images, labels = data\n",
    "  #     images = images.cuda()\n",
    "  #     labels = labels.cuda()\n",
    "  #     predicted_output = model(images)\n",
    "  #     fit = loss(predicted_output, labels)\n",
    "  #     test_loss += fit.item()\n",
    "  #     _, predicted = torch.max(predicted_output.data, 1)\n",
    "  #     matches += (predicted == labels).sum().item()\n",
    "  #     total += labels.size(0)\n",
    "  # train_loss = train_loss / len(trainDataLoader)\n",
    "  # test_loss = test_loss / len(testDataLoader)\n",
    "  # train_loss_history += [train_loss]\n",
    "  # test_loss_history += [test_loss]\n",
    "  # print(f'Epoch {epoch}, Train loss {train_loss}, Test loss {test_loss}')\n",
    "    # scheduler.step()\n",
    "# train_loss_history = []\n",
    "# test_loss_history = []\n",
    "# matches = 0\n",
    "# total = 0\n",
    "\n",
    "\n",
    "# for epoch in range(80):\n",
    "#   train_loss = 0.0\n",
    "#   test_loss = 0.0\n",
    "\n",
    "#   model.train()\n",
    "#   for i, data in enumerate(trainDataLoader):\n",
    "#     images, labels = data\n",
    "#     images = images.cuda()\n",
    "#     labels = labels.cuda()\n",
    "#     optimizer.zero_grad() # zero out any gradient values from the previous iteration\n",
    "#     predicted_output = model(images) # forward propagation\n",
    "#     fit = loss(predicted_output, labels)  # calculate our measure of goodness\n",
    "#     fit.backward() # backpropagation\n",
    "#     optimizer.step() # update the weights of our trainable parameters\n",
    "#     train_loss += fit.item()\n",
    "\n",
    "#   model.eval()\n",
    "#   for i, data in  enumerate(testDataLoader):\n",
    "#     with torch.no_grad():\n",
    "#       images, labels = data\n",
    "#       images = images.cuda()\n",
    "#       labels = labels.cuda()\n",
    "#       predicted_output = model(images)\n",
    "#       fit = loss(predicted_output, labels)\n",
    "#       test_loss += fit.item()\n",
    "#       _, predicted = torch.max(predicted_output.data, 1)\n",
    "#       matches += (predicted == labels).sum().item()\n",
    "#       total += labels.size(0)\n",
    "#   train_loss = train_loss / len(trainDataLoader)\n",
    "#   test_loss = test_loss / len(testDataLoader)\n",
    "#   train_loss_history += [train_loss]\n",
    "#   test_loss_history += [test_loss]\n",
    "#   # scheduler.step()\n",
    "#   print(f'Epoch {epoch}, Train loss {train_loss}, Test loss {test_loss}')\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bb5c744-cb72-4dd6-af26-7534605d1530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 80.91%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct, total = 0, 0\n",
    "    for images, labels in testDataLoader:\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f'Test accuracy: {100 * correct / total}%')\n",
    "\n",
    "# ~10 epochs = 81.5%?\n",
    "# ~13 = Test accuracy: 79.6%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "714c0c88-051b-46d8-b4bd-1e96119dc86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "# CHECK ACCURACY \n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "test_images_nl = unpickle('cifar_test_nolabels.pkl')[b'data']\n",
    "test_images_nl2 = unpickle('cifar_test_nolabels.pkl')[b'data']\n",
    "test_images_nl_id = unpickle('cifar_test_nolabels.pkl')[b'ids'].tolist()\n",
    "\n",
    "test_images_nl = test_images_nl.reshape((-1, 3, 32, 32))\n",
    "min_val = np.min(test_images_nl)\n",
    "max_val = np.max(test_images_nl)\n",
    "test_images_nl = (test_images_nl - min_val) / (max_val - min_val) #normalizing facepalm\n",
    "\n",
    "test_images_nl2 = test_images_nl2.reshape((-1, 3, 32, 32)).astype(np.float32) / 255.0\n",
    "\n",
    "# print(test_images_nl)\n",
    "# print(\"rawr\")\n",
    "# print(test_images_nl2)\n",
    "\n",
    "# plt.figure(figsize=(20, 4))\n",
    "# for i in range(10):\n",
    "#     plt.subplot(1, 10, i+1)\n",
    "#     plt.imshow(test_images_nl[i])\n",
    "#     plt.axis('off')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "test_images_tensor_nl = torch.tensor(test_images_nl, dtype=torch.float32)\n",
    "testDataLoaderNL = torch.utils.data.DataLoader(test_images_tensor_nl, batch_size=1,shuffle=False)\n",
    "\n",
    "\n",
    "predictions = []\n",
    "model.eval()\n",
    "for image in testDataLoaderNL:\n",
    "    image = image.cuda()\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "\n",
    "    # _, predicted_class = torch.max(output, 1)\n",
    "    # predictions.append(predicted_class.item())\n",
    "\n",
    "    # Preprocess the image if necessary\n",
    "    # Perform inference\n",
    "    # Here we assume 'resnet' is already loaded and 'image' is preprocessed\n",
    "    output = model(image)\n",
    "    # Perform post-processing if necessary\n",
    "    # Append the prediction to the predictions list\n",
    "    predictions.append(output.argmax().item())\n",
    "\n",
    "print(len(predictions))\n",
    "# # print(test_images_nl_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1fb892f5-aff6-4dbe-b216-3472e380e00c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.41"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_accuracy(predictions, actual):\n",
    "    if len(predictions) != len(actual):\n",
    "        raise ValueError(\"Length of predictions and actual arrays must be the same.\")\n",
    "    \n",
    "    num_matches = sum(1 for pred, act in zip(predictions, actual) if pred == act)\n",
    "    accuracy = (num_matches / len(predictions)) * 100\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "actual = []\n",
    "pattern = [8, 2, 9, 0, 4, 3, 6, 1, 7, 5]\n",
    "for num in pattern:\n",
    "    actual.extend([num] * 1000)\n",
    "calculate_accuracy(predictions, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ecf71348-271c-4e9b-9bd1-403e6708353b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with IDs and labels\n",
    "df = pd.DataFrame({\n",
    "    'ID': test_images_nl_id,\n",
    "    'Labels': predictions\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('predictions.csv', index=False)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "111291bb-d547-4ca7-ae98-ca081e95162f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of group 1: 7.265 ,, SHOULD BE: 8\n",
      "[8, 8, 8, 8, 8, 8, 8, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 3, 8, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 8, 8, 3]\n",
      "Average of group 2: 2.531 ,, SHOULD BE: 2\n",
      "[3, 2, 4, 2, 5, 2, 2, 3, 6, 2, 3, 2, 2, 2, 0, 4, 2, 2, 0, 0, 2, 8, 3, 2, 7, 2, 0, 2, 2, 2, 2, 2, 3, 2, 2, 0, 5, 2, 2, 3, 2, 0, 2, 2, 2, 2, 3, 4, 4, 2]\n",
      "Average of group 3: 6.749 ,, SHOULD BE: 9\n",
      "[9, 6, 8, 9, 9, 9, 8, 9, 8, 3, 4, 1, 8, 9, 9, 9, 1, 9, 8, 9, 8, 9, 3, 9, 1, 8, 9, 3, 9, 3, 8, 9, 9, 1, 3, 9, 9, 8, 9, 9, 9, 4, 9, 9, 1, 8, 8, 3, 9, 9]\n",
      "Average of group 4: 2.932 ,, SHOULD BE: 0\n",
      "[0, 0, 8, 8, 0, 8, 2, 3, 0, 8, 0, 9, 8, 0, 0, 3, 3, 4, 8, 2, 0, 0, 0, 2, 0, 0, 0, 8, 8, 5, 0, 0, 0, 8, 0, 8, 0, 0, 8, 0, 4, 0, 0, 8, 8, 0, 0, 8, 0, 3]\n",
      "Average of group 5: 3.651 ,, SHOULD BE: 4\n",
      "[7, 4, 4, 3, 4, 4, 3, 3, 4, 4, 4, 2, 0, 3, 7, 3, 4, 4, 4, 3, 4, 4, 0, 9, 8, 2, 4, 0, 3, 4, 0, 2, 2, 4, 4, 4, 4, 2, 9, 4, 4, 4, 7, 3, 4, 4, 4, 5, 4, 4]\n",
      "Average of group 6: 3.4 ,, SHOULD BE: 3\n",
      "[4, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 3, 5, 3, 3, 5, 3, 8, 3, 2, 3, 3, 5, 3, 5, 9, 0, 5, 5, 8, 3, 5, 3, 5, 3, 5, 8, 3, 3, 3, 5, 5, 3, 0, 3, 5, 3, 2]\n",
      "Average of group 7: 4.148 ,, SHOULD BE: 6\n",
      "[6, 6, 3, 5, 2, 0, 3, 4, 0, 3, 3, 3, 0, 3, 0, 4, 4, 4, 3, 2, 6, 3, 8, 6, 0, 2, 3, 6, 6, 4, 3, 2, 6, 6, 3, 6, 2, 2, 2, 3, 3, 3, 3, 3, 4, 3, 6, 3, 0, 5]\n",
      "Average of group 8: 4.166 ,, SHOULD BE: 1\n",
      "[5, 0, 1, 2, 8, 7, 1, 2, 8, 9, 9, 8, 9, 3, 0, 1, 1, 0, 8, 9, 9, 0, 3, 8, 5, 8, 3, 9, 1, 1, 1, 0, 1, 1, 9, 1, 5, 1, 1, 1, 3, 9, 5, 3, 8, 1, 1, 9, 0, 8]\n",
      "Average of group 9: 4.979 ,, SHOULD BE: 7\n",
      "[7, 7, 7, 7, 4, 3, 4, 4, 7, 7, 4, 4, 4, 2, 0, 5, 7, 4, 7, 7, 0, 7, 7, 2, 4, 5, 7, 7, 7, 7, 2, 4, 4, 3, 4, 4, 0, 4, 8, 4, 3, 2, 7, 4, 7, 7, 4, 7, 7, 7]\n",
      "Average of group 10: 4.152 ,, SHOULD BE: 5\n",
      "[5, 5, 5, 5, 5, 5, 5, 5, 3, 5, 3, 5, 5, 3, 5, 3, 0, 5, 0, 3, 2, 3, 3, 9, 5, 5, 5, 5, 4, 8, 2, 4, 2, 2, 3, 5, 5, 5, 5, 5, 3, 5, 5, 4, 5, 4, 5, 5, 5, 5]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the number of groups\n",
    "num_groups = 10\n",
    "\n",
    "# Calculate the size of each group\n",
    "group_size = len(predictions) // num_groups\n",
    "\n",
    "# print(predictions[:50]) #8\n",
    "# print(predictions[1000:1050]) #2\n",
    "# print(predictions[2000:2050]) #9\n",
    "# print(predictions[3000:3050]) #0\n",
    "# print(predictions[4000:4050]) #4\n",
    "# print(predictions[5000:5050]) #3\n",
    "# print(predictions[6000:6050]) #6\n",
    "# print(predictions[7000:7050]) #1\n",
    "# print(predictions[8000:8050]) #7\n",
    "# print(predictions[9000:9050]) #5\n",
    "should = {\n",
    "    0: ['8', predictions[:50]],\n",
    "    1: ['2', predictions[1000:1050]],\n",
    "    2: ['9', predictions[2000:2050]],\n",
    "    3: ['0', predictions[3000:3050]],\n",
    "    4: ['4', predictions[4000:4050]],\n",
    "    5: ['3', predictions[5000:5050]],\n",
    "    6: ['6', predictions[6000:6050]],\n",
    "    7: ['1', predictions[7000:7050]],\n",
    "    8: ['7', predictions[8000:8050]],\n",
    "    9: ['5', predictions[9000:9050]]\n",
    "}\n",
    "\n",
    "\n",
    "# Loop through each group\n",
    "for i in range(num_groups):\n",
    "    # Determine the start and end indices for the current group\n",
    "    start_index = i * group_size\n",
    "    end_index = (i + 1) * group_size\n",
    "\n",
    "    # Slice the predictions for the current group\n",
    "    group_predictions = predictions[start_index:end_index]\n",
    "\n",
    "    # Calculate the average of the current group\n",
    "    average = sum(group_predictions) / len(group_predictions)\n",
    "\n",
    "    # Print the average of the current group\n",
    "    print(f\"Average of group {i + 1}: {average} ,, SHOULD BE: \" + should[i][0])\n",
    "    print(should[i][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9dd461a-d42c-44de-894b-242d4bf2b535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 8, 8, 8, 8, 8, 8, 3, 8, 8, 8, 4, 8, 8, 8, 8, 6, 0, 8, 0, 8, 0, 8, 8, 8, 8, 8, 8, 8, 0, 8, 8, 3, 8, 4, 8, 8, 8, 8, 8, 8, 8, 8, 0, 8, 8, 8, 8, 8, 8]\n",
      "[3, 3, 4, 3, 3, 3, 0, 0, 3, 3, 0, 3, 5, 2, 0, 4, 3, 0, 0, 3, 3, 0, 3, 3, 4, 4, 0, 4, 2, 0, 3, 0, 3, 0, 4, 3, 3, 3, 0, 4, 4, 2, 2, 0, 2, 3, 4, 3, 0, 2]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ec91a28-51e5-4c5d-ba73-96b4719721a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The device is cuda\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torchsummary import summary\n",
    "# # guesses 8 foreverf\n",
    "import os\n",
    "\n",
    "train_losses, test_losses, train_accs, test_accs, best_acc = None, None, None, None, None\n",
    "\n",
    "model = ResNet(BasicBlock, [2,2,2,2])\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"The device is {device}\")\n",
    "model = model.to(device)\n",
    "if device == 'cuda':\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), \n",
    "                      lr=1e-4, \n",
    "                      momentum=0.9, \n",
    "                      weight_decay=5e-2)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d'%epoch)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct, total = 0, 0\n",
    "    for (inputs, targets) in trainDataLoader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    acc = correct/total*100\n",
    "    print(f\"Train | Loss: {round(train_loss, 4)} | Acc: {round(acc, 2)}\")\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(acc)\n",
    "def test(epoch):\n",
    "    global train_losses, test_losses, train_accs, test_accs, best_acc\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for (inputs, targets) in testDataLoader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    acc = correct/total*100\n",
    "    print(f\"Test  | Loss: {round(test_loss, 4)} | Acc: {round(acc, 2)}\")\n",
    "    test_losses.append(test_loss)\n",
    "    test_accs.append(acc)\n",
    "    \n",
    "    state = {'model': model.state_dict(),\n",
    "             'train_loss': train_losses,\n",
    "             'test_loss': test_losses,\n",
    "             'train_acc': train_accs,\n",
    "             'test_acc': test_accs,\n",
    "             'epoch': epoch}\n",
    "    torch.save(state, './checkpoint/ckpt.pth')\n",
    "    if acc > best_acc:\n",
    "        torch.save(state, './checkpoint/ckpt_best.pth')\n",
    "        best_acc = acc\n",
    "if not os.path.isdir('checkpoint'):\n",
    "    os.mkdir('checkpoint')\n",
    "state = {'model': model.state_dict(),\n",
    "         'train_loss': [],\n",
    "         'test_loss': [],\n",
    "         'train_acc': [],\n",
    "         'test_acc': [],\n",
    "         'epoch': -1}\n",
    "torch.save(state, './checkpoint/ckpt.pth')\n",
    "torch.save(state, './checkpoint/ckpt_best.pth')\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c096bc69-642c-4972-982b-64cea3458ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Train | Loss: 825.7793 | Acc: 21.96\n",
      "Test  | Loss: 149.9225 | Acc: 29.94\n",
      "\n",
      "Epoch: 1\n",
      "Train | Loss: 718.3924 | Acc: 32.43\n",
      "Test  | Loss: 133.4376 | Acc: 37.36\n",
      "\n",
      "Epoch: 2\n",
      "Train | Loss: 664.8541 | Acc: 37.39\n",
      "Test  | Loss: 126.1092 | Acc: 40.97\n",
      "\n",
      "Epoch: 3\n",
      "Train | Loss: 628.7557 | Acc: 41.03\n",
      "Test  | Loss: 119.1432 | Acc: 43.76\n",
      "\n",
      "Epoch: 4\n",
      "Train | Loss: 596.5287 | Acc: 44.18\n",
      "Test  | Loss: 113.2606 | Acc: 46.78\n",
      "\n",
      "Epoch: 5\n",
      "Train | Loss: 566.9384 | Acc: 47.33\n",
      "Test  | Loss: 109.5175 | Acc: 48.28\n",
      "\n",
      "Epoch: 6\n",
      "Train | Loss: 541.0119 | Acc: 49.98\n",
      "Test  | Loss: 105.8996 | Acc: 50.05\n",
      "\n",
      "Epoch: 7\n",
      "Train | Loss: 515.8529 | Acc: 52.47\n",
      "Test  | Loss: 102.3634 | Acc: 51.91\n",
      "\n",
      "Epoch: 8\n",
      "Train | Loss: 492.9617 | Acc: 54.78\n",
      "Test  | Loss: 98.3274 | Acc: 54.24\n",
      "\n",
      "Epoch: 9\n",
      "Train | Loss: 468.0277 | Acc: 57.18\n",
      "Test  | Loss: 95.0688 | Acc: 56.22\n",
      "\n",
      "Epoch: 10\n",
      "Train | Loss: 445.7654 | Acc: 59.71\n",
      "Test  | Loss: 98.6987 | Acc: 55.19\n",
      "\n",
      "Epoch: 11\n",
      "Train | Loss: 420.9007 | Acc: 61.99\n",
      "Test  | Loss: 89.3721 | Acc: 59.84\n",
      "\n",
      "Epoch: 12\n",
      "Train | Loss: 399.8153 | Acc: 64.03\n",
      "Test  | Loss: 89.6898 | Acc: 59.41\n",
      "\n",
      "Epoch: 13\n",
      "Train | Loss: 374.9924 | Acc: 66.68\n",
      "Test  | Loss: 89.5542 | Acc: 60.0\n",
      "\n",
      "Epoch: 14\n",
      "Train | Loss: 351.3916 | Acc: 68.97\n",
      "Test  | Loss: 84.8883 | Acc: 61.38\n",
      "\n",
      "Epoch: 15\n",
      "Train | Loss: 326.5942 | Acc: 71.44\n",
      "Test  | Loss: 86.2208 | Acc: 61.44\n",
      "\n",
      "Epoch: 16\n",
      "Train | Loss: 301.8888 | Acc: 74.15\n",
      "Test  | Loss: 87.9587 | Acc: 61.19\n",
      "\n",
      "Epoch: 17\n",
      "Train | Loss: 273.8936 | Acc: 76.77\n",
      "Test  | Loss: 88.9188 | Acc: 61.26\n",
      "\n",
      "Epoch: 18\n",
      "Train | Loss: 245.9101 | Acc: 79.89\n",
      "Test  | Loss: 88.1074 | Acc: 62.67\n",
      "\n",
      "Epoch: 19\n",
      "Train | Loss: 218.3828 | Acc: 82.77\n",
      "Test  | Loss: 89.1866 | Acc: 62.31\n",
      "\n",
      "Epoch: 20\n",
      "Train | Loss: 187.7361 | Acc: 86.08\n",
      "Test  | Loss: 91.8026 | Acc: 62.87\n",
      "\n",
      "Epoch: 21\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m start_epoch \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, start_epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m71\u001b[39m):\n\u001b[0;32m---> 11\u001b[0m     train(epoch)\n\u001b[1;32m     12\u001b[0m     test(epoch)\n\u001b[1;32m     13\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[8], line 39\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     36\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 39\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     40\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     41\u001b[0m total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "checkpoint = torch.load('./checkpoint/ckpt.pth')\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "train_losses = checkpoint['train_loss']\n",
    "test_losses = checkpoint['test_loss']\n",
    "train_accs = checkpoint['train_acc']\n",
    "test_accs = checkpoint['test_acc']\n",
    "best_acc = 0.0 if not test_accs else max(test_accs)\n",
    "start_epoch = checkpoint['epoch']\n",
    "\n",
    "for epoch in range(start_epoch + 1, start_epoch + 71):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a05c073c-a84a-4d6a-a789-49ebbd27ca56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize = (10,3))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(len(train_losses)), train_losses, label = 'Train loss')\n",
    "plt.plot(range(len(test_losses)), test_losses, label = 'Val loss')\n",
    "plt.title('Loss curve')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(len(train_losses)), train_accs, label = 'Train acc')\n",
    "plt.plot(range(len(test_losses)), test_accs, label = 'Val acc')\n",
    "plt.title('Accuracy curve')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30767f9a-9cd8-43b2-a158-158ae2881b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b72f8234-6a83-4e43-b73a-9fb9658e2243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:  11173962\n",
      "Epoch 0, Train loss 1.9902066746941003, Test loss 1.7035919427871704\n",
      "Epoch 1, Train loss 1.6745598771992851, Test loss 1.5216001477211145\n",
      "Epoch 2, Train loss 1.5205875904968633, Test loss 1.403229348978419\n",
      "Epoch 3, Train loss 1.400783354668971, Test loss 1.3252996137947033\n",
      "Epoch 4, Train loss 1.2948132839501667, Test loss 1.2826783414099627\n",
      "Epoch 5, Train loss 1.1978501484674566, Test loss 1.3470595858658954\n",
      "Epoch 6, Train loss 1.099954070718697, Test loss 1.6947776830879746\n",
      "Epoch 7, Train loss 1.0096933765484548, Test loss 1.467735649673802\n",
      "Epoch 8, Train loss 0.916751261105013, Test loss 1.312023409612619\n",
      "Epoch 9, Train loss 0.8189468559096841, Test loss 1.3116138581257717\n",
      "Epoch 10, Train loss 0.7134648416658191, Test loss 1.388568262006067\n",
      "Epoch 11, Train loss 0.6071600317573913, Test loss 1.3607884546753708\n",
      "Epoch 12, Train loss 0.4968853428616853, Test loss 1.3561915333863277\n",
      "Epoch 13, Train loss 0.3926280567903653, Test loss 2.4584409294614367\n",
      "Epoch 14, Train loss 0.29183694710740654, Test loss 1.6895658149840727\n",
      "Epoch 15, Train loss 0.21436321929745053, Test loss 2.4743491471952694\n",
      "Epoch 16, Train loss 0.14871731070834962, Test loss 1.492080330089399\n",
      "Epoch 17, Train loss 0.10145647680896627, Test loss 2.5090165958283053\n",
      "Epoch 18, Train loss 0.07443895398417626, Test loss 1.6686334666932465\n",
      "Epoch 19, Train loss 0.052846805909481806, Test loss 2.262073361949556\n",
      "Epoch 20, Train loss 0.03950985051129404, Test loss 1.7396404397715428\n",
      "Epoch 21, Train loss 0.029525177731938527, Test loss 2.081226887976288\n",
      "Epoch 22, Train loss 0.024030655617718503, Test loss 1.899296573772552\n",
      "Epoch 23, Train loss 0.019416453115537267, Test loss 3.1711485780728093\n",
      "Epoch 24, Train loss 0.018506810881907258, Test loss 1.836211949776692\n",
      "Epoch 25, Train loss 0.014021962292759163, Test loss 1.7531657644138214\n",
      "Epoch 26, Train loss 0.013708481012814966, Test loss 2.735960284615778\n",
      "Epoch 27, Train loss 0.013378098412938511, Test loss 1.892476578806616\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m   fit\u001b[38;5;241m.\u001b[39mbackward() \u001b[38;5;66;03m# backpropagation\u001b[39;00m\n\u001b[1;32m     39\u001b[0m   optimizer\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m# update the weights of our trainable parameters\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m   train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m fit\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     42\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m  \u001b[38;5;28menumerate\u001b[39m(testDataLoader):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2217c918-2781-4217-8db9-f5b7ca451a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 60 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "for i, data in  enumerate(testDataLoader):\n",
    "  images, labels = data\n",
    "  images = images.cuda()\n",
    "  labels = labels.cuda()\n",
    "  with torch.no_grad():\n",
    "    predicted_output = model(images)\n",
    "    _, predicted = torch.max(predicted_output.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print('Accuracy of the network on the test images: %d %%' % (100 * accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb551f8f-cb34-4fc3-827c-7ec349bbc3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet34().cuda()\n",
    "loss = torch.nn.CrossEntropyLoss() # Step 2: loss\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=.01)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Number of parameters: \", num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c288674c-71ae-4ada-9799-112c681b7abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6, 4, 3, 6, 6, 2, 6, 3, 5, 4, 0, 0, 9, 1, 3, 4, 0, 3, 7, 3, 3, 5, 2, 2, 7, 1, 1, 1, 2, 2, 0, 9, 5, 7, 9, 2, 2, 5, 2, 4, 3, 1, 1, 8, 2, 1, 1, 4, 9, 7, 8, 5, 9, 6, 7, 3, 1, 9, 0, 3, 1, 3, 5, 4, 5, 7, 7, 4, 7, 9, 4, 2, 3, 8, 0, 1, 6, 1, 1, 4, 1, 8, 3, 9, 6, 6, 1, 8, 5, 2, 9, 9, 8, 1, 7, 7, 0, 0, 6, 9, 1, 2, 2, 9, 2, 6, 6, 1, 9, 5, 0, 4, 7, 6, 7, 1, 8, 1, 1, 2, 8, 1, 3, 3, 6, 2, 4, 9, 9, 5, 4, 3, 6, 7, 4, 6, 8, 5, 5, 4, 3, 1, 8, 4, 7, 6, 0, 9, 5, 1, 3, 8, 2, 7, 5, 3, 4, 1, 5, 7, 0, 4, 7, 5, 5, 1, 0, 9, 6, 9, 0, 8, 7, 8, 8, 2, 5, 2, 3, 5, 0, 6, 1, 9, 3, 6, 9, 1, 3, 9, 6, 6, 7, 1, 0, 9, 5, 8, 5, 2, 9, 0, 8, 8, 0, 6, 9, 1, 1, 6, 3, 7, 6, 6, 0, 6, 6, 1, 7, 1, 5, 8, 3, 6, 6, 8, 6, 8, 4, 6, 6, 1, 3, 8, 3, 4, 1, 7, 1, 3, 8, 5, 1, 1, 4, 0, 9, 3, 7, 4, 9, 9, 2, 4, 9, 9, 1, 0, 5, 9, 0, 8, 2, 1, 2, 0, 5, 6, 3, 2, 7, 8, 8, 6, 0, 7, 9, 4, 5, 6, 4, 2, 1, 1, 2, 1, 5, 9, 9, 0, 8, 4, 1, 1, 6, 3, 3, 9, 0, 7, 9, 7, 7, 9, 1, 5, 1, 6, 6, 8, 7, 1, 3, 0, 3, 3, 2, 4, 5, 7, 5, 9, 0, 3, 4, 0, 4, 4, 6, 0, 0, 6, 6, 0, 8, 1, 6, 2, 9, 2, 5, 9, 6, 7, 4, 1, 8, 7, 3, 6, 9, 3, 0, 4, 0, 5, 1, 0, 3, 4, 8, 5, 4, 7, 2, 3, 9, 7, 6, 7, 1, 4, 7, 0, 1, 7, 3, 1, 8, 4, 4, 2, 0, 2, 2, 0, 0, 9, 0, 9, 6, 8, 2, 7, 7, 4, 0, 3, 0, 8, 9, 4, 2, 7, 2, 5, 2, 5, 1, 9, 4, 8, 5, 1, 7, 4, 4, 0, 6, 9, 0, 7, 8, 8, 9, 9, 3, 3, 4, 0, 4, 5, 6, 6, 0, 1, 0, 8, 0, 4, 8, 8, 1, 5, 2, 6, 8, 1, 0, 0, 7, 7, 5, 9, 6, 2, 8, 3, 4, 7, 3, 9, 0, 1, 2, 4, 8, 1, 8, 6, 4, 4, 5, 7, 1, 3, 9, 8, 0, 1, 7, 5, 8, 2, 8, 0, 4, 1, 8, 9, 8, 2, 9, 9, 2, 7, 5, 7, 3, 8, 8, 4, 4, 2, 7, 1, 6, 4, 0, 4, 6, 9, 7, 6, 2, 5, 5, 1, 7, 2, 2, 2, 9, 5, 4, 2, 7, 8, 1, 3, 4, 3, 7, 6, 9, 8, 0, 6, 0, 2, 2, 2, 1, 8, 4, 0, 1, 8, 8, 1, 5, 7, 6, 4, 5, 8, 7, 1, 9, 1, 9, 8, 4, 7, 3, 8, 8, 2, 6, 6, 7, 1, 6, 8, 1, 9, 7, 8, 3, 0, 1, 0, 8, 8, 3, 0, 0, 1, 5, 0, 8, 8, 7, 9, 9, 0, 9, 4, 1, 3, 6, 6, 4, 4, 7, 5, 6, 0, 8, 0, 3, 2, 8, 4, 6, 9, 9, 7, 0, 3, 3, 6, 7, 4, 9, 1, 6, 2, 7, 2, 2, 0, 6, 7, 5, 7, 6, 8, 9, 0, 9, 4, 4, 7, 0, 9, 4, 9, 6, 9, 4, 5, 7, 9, 2, 4, 5, 1, 4, 3, 9, 6, 5, 6, 9, 3, 3, 5, 0, 7, 2, 1, 3, 6, 4, 0, 0, 2, 5, 0, 1, 0, 2, 3, 9, 8, 4, 9, 8, 0, 2, 6, 4, 4, 0, 1, 8, 8, 3, 6, 9, 6, 6, 7, 8, 2, 4, 5, 7, 6, 5, 3, 0, 5, 0, 5, 0, 8, 2, 6, 7, 3, 8, 2, 1, 7, 6, 7, 1, 0, 9, 5, 5, 0, 1, 7, 6, 9, 0, 4, 7, 7, 1, 5, 9, 4, 0, 8, 5, 9, 9, 6, 7, 1, 8, 3, 2, 3, 8, 2, 2, 4, 6, 0, 0, 5, 3, 8, 2, 3, 7, 2, 9, 3, 8, 7, 8, 2, 7, 9, 0, 2, 3, 2, 2, 2, 3, 3, 6, 2, 3, 2, 8, 0, 5, 5, 1, 4, 5, 6, 6, 2, 7, 0, 1, 7, 7, 8, 2, 9, 2, 2, 4, 2, 1, 1, 1, 6, 6, 6, 5, 1, 1, 7, 0, 4, 3, 3, 7, 1, 2, 3, 5, 5, 5, 6, 1, 4, 3, 7, 8, 8, 3, 6, 6, 2, 3, 0, 9, 4, 3, 8, 0, 0, 1, 1, 5, 4, 9, 3, 1, 8, 9, 3, 9, 9, 2, 9, 4, 8, 2, 9, 8, 8, 1, 5, 3, 6, 8, 7, 6, 9, 8, 0, 6, 4, 0, 0, 2, 5, 8, 2, 0, 2, 7, 6, 9, 7, 1, 5, 5, 6, 6, 3, 6, 2, 4, 7, 0, 5, 6, 4, 6, 5, 2, 4, 6, 1, 6, 0, 4, 0, 3, 1, 8, 5, 4, 4, 1, 7, 3, 9, 4, 7, 9, 7, 3, 7, 2, 8, 4, 6, 6, 1, 2, 9, 0, 4, 8, 7, 3, 9, 8, 7, 7, 0, 2, 4, 1, 1, 4, 1, 5, 4, 0, 5, 6, 2, 8, 5, 0, 2, 1, 3, 5, 7, 3, 5, 1, 3, 5, 9, 4, 3, 2, 4, 4, 1, 4, 2, 2, 3, 8, 0, 6, 8, 5, 6, 6, 4, 7, 1, 1, 4, 6, 2, 3, 9, 6, 9, 1, 3, 6, 8, 5, 9, 6, 8, 1, 6, 0, 2, 3, 7, 9, 0, 9, 7, 6, 3, 9, 2, 6, 1, 6, 7, 3, 8, 3, 8, 3, 8, 5, 9, 6, 1, 2, 5, 2, 1, 4, 3, 7, 5, 9, 3, 9, 3, 2, 9, 1, 8, 5, 9, 7, 2, 6, 0, 8, 5, 7, 1, 5, 8, 5, 7, 1, 5, 0, 3, 9, 3, 6, 9, 1, 3, 9, 8, 2, 2, 3, 2, 5, 9, 7, 9, 9, 8, 9, 7, 0, 3, 3, 2, 0, 3, 7, 6, 3, 3, 2, 0, 6, 6, 5, 5, 7, 5, 9, 8, 2, 9, 8, 0, 4, 0, 1, 2, 0, 4, 7, 3, 8, 5, 1, 6, 6, 5, 5, 4, 6, 3, 6, 8, 2, 3, 7, 0, 7, 0, 4, 1, 9, 5, 7, 8, 6, 6, 8, 0, 7, 2, 8, 4, 8, 2, 0, 9, 0, 0, 2, 9, 6, 6, 5, 6, 0, 3, 7, 5, 5, 7, 9, 3, 4, 5, 0, 5, 2, 3, 2, 6, 0, 4, 9, 0, 7, 0, 9, 7, 2, 6, 4, 6, 9, 5, 4, 7, 0, 6, 8, 8, 9, 9, 9, 0, 9, 8, 6, 4, 8, 1, 9, 1, 0, 5, 8, 6, 9, 6, 0, 8, 1, 3, 9, 4, 8, 4, 3, 2, 6, 0, 8, 9, 9, 4, 3, 0, 2, 4, 4, 0, 3, 5, 7, 5, 7, 7, 9, 0, 9, 5, 3, 8, 2, 4, 2, 3, 1, 2, 8, 9, 2, 8, 1, 4, 2, 0, 4, 5, 4, 8, 1, 7, 4, 1, 1, 0, 2, 7, 7, 4, 4, 4, 4, 8, 4, 3, 6, 6, 0, 1, 3, 9, 8, 4, 8, 9, 6, 2, 0, 5, 5, 9, 4, 2, 0, 8, 8, 0, 4, 0, 7, 6, 9, 5, 3, 5, 4, 4, 4, 4, 3, 7, 9, 2, 5, 1, 8, 3, 2, 6, 9, 6, 3, 1, 7, 4, 6, 3, 7, 8, 6, 2, 4, 6, 8, 0, 1, 9, 9, 1, 0, 0, 8, 9, 4, 7, 4, 4, 1, 9, 8, 8, 6, 1, 7, 4, 8, 8, 8, 0, 5, 6, 6, 8, 3, 4, 4, 1, 2, 1, 5, 7, 1, 7, 2, 8, 5, 9, 5, 6, 1, 9, 5, 0, 4, 3, 3, 0, 8, 2, 8, 0, 9, 0, 4, 6, 9, 2, 8, 2, 7, 7, 2, 2, 7, 1, 6, 1, 3, 4, 4, 8, 6, 0, 1, 9, 4, 2, 7, 5, 3, 9, 0, 1, 9, 0, 9, 8, 7, 0, 4, 9, 0, 5, 2, 1, 0, 2, 8, 8, 0, 5, 7, 6, 6, 5, 3, 7, 7, 7, 4, 2, 0, 1, 7, 3, 2, 7, 3, 2, 2, 1, 5, 9, 9, 8, 0, 8, 4, 3, 3, 8, 5, 0, 8, 4, 5, 7, 5, 1, 5, 0, 2, 0, 5, 4, 2, 3, 6, 2, 6, 2, 2, 3, 4, 6, 6, 5, 3, 0, 1, 2, 7, 7, 5, 3, 1, 1, 7, 6, 1, 6, 3, 3, 3, 3, 4, 8, 1, 0, 7, 7, 6, 8, 1, 2, 5, 3, 4, 1, 1, 9, 3, 1, 4, 2, 7, 1, 6, 5, 7, 7, 7, 6, 6, 1, 9, 5, 0, 9, 5, 7, 0, 0, 4, 6, 8, 0, 1, 5, 0, 9, 3, 1, 1, 2, 0, 3, 2, 9, 1, 9, 6, 2, 4, 8, 8, 5, 9, 2, 1, 9, 4, 4, 3, 0, 6, 6, 0, 4, 1, 0, 9, 5, 6, 5, 6, 9, 4, 4, 2, 6, 8, 4, 7, 6, 5, 9, 8, 7, 1, 9, 5, 4, 3, 5, 4, 3, 4, 1, 5, 5, 4, 0, 8, 4, 4, 0, 9, 2, 8, 9, 8, 0, 2, 2, 2, 6, 7, 8, 1, 9, 8, 0, 3, 8, 6, 8, 1, 6, 5, 4, 2, 1, 4, 3, 9, 7, 8, 3, 0, 8, 3, 4, 2, 9, 1, 0, 0, 3, 0, 4, 5, 9, 0, 7, 5, 9, 5, 8, 8, 6, 3, 1, 9, 5, 2, 4, 7, 6, 1, 8, 6, 9, 3, 1, 3, 7, 4, 0, 6, 7, 6, 9, 2, 4, 1, 9, 8, 5, 8, 2, 2, 5, 0, 2, 0, 7, 0, 6, 6, 4, 8, 7, 9, 6, 9, 2, 3, 8, 8, 3, 9, 9, 8, 7, 2, 3, 5, 5, 1, 8, 7, 4, 3, 5, 2, 2, 2, 1, 4, 2, 2, 8, 4, 9, 8, 2, 1, 2, 5, 6, 3, 4, 5, 7, 6, 7, 6, 5, 0, 2, 4, 4, 3, 0, 4, 2, 6, 1, 8, 8, 3, 0, 7, 4, 9, 7, 9, 2, 0, 4, 7, 3, 7, 6, 6, 2, 3, 7, 3, 6, 8, 2, 3, 3, 5, 5, 5, 2, 4, 2, 8, 7, 4, 3, 7, 7, 8, 5, 2, 8, 4, 3, 5, 4, 2, 9, 1, 4, 0, 0, 5, 6, 5, 6, 8, 0, 3, 4, 4, 4, 2, 2, 0, 5, 0, 3, 4, 7, 7, 3, 9, 7, 3, 7, 3, 7, 7, 7, 1, 7, 4, 4, 7, 9, 1, 7, 7, 4, 5, 9, 0, 8, 7, 3, 6, 2, 3, 8, 2, 5, 4, 8, 4, 0, 7, 5, 2, 7, 7, 2, 6, 4, 0, 2, 4, 3, 8, 9, 4, 5, 5, 6, 7, 1, 9, 6, 5, 0, 3, 4, 4, 0, 6, 6, 8, 3, 6, 0, 3, 3, 3, 8, 3, 3, 8, 4, 3, 8, 2, 9, 1, 4, 9, 5, 0, 6, 5, 0, 2, 6, 5, 4, 1, 5, 9, 0, 6, 2, 5, 4, 5, 8, 2, 8, 7, 5, 0, 7, 9, 9, 8, 2, 1, 0, 2, 7, 5, 0, 3, 8, 5, 0, 7, 9, 5, 8, 6, 9, 0, 7, 1, 9, 3, 5, 0, 5, 1, 7, 8, 9, 2, 0, 0, 8, 1, 1, 1, 6, 2, 3, 6, 2, 7, 1, 9, 3, 7, 6, 3, 1, 0, 5, 9, 4, 3, 9, 5, 0, 9, 2, 7, 5, 8, 0, 1, 3, 4, 8, 5, 1, 5, 7, 7, 2, 8, 4, 2, 2, 5, 3, 3, 1, 4, 9, 2, 4, 5, 2, 2, 3, 2, 1, 5, 8, 9, 2, 6, 6, 1, 7, 7, 5, 4, 4, 0, 5, 8, 8, 6, 6, 7, 2, 6, 4, 5, 1, 7, 5, 2, 4, 4, 6, 5, 2, 3, 8, 9, 3, 4, 3, 2, 6, 7, 2, 3, 2, 7, 7, 3, 0, 1, 4, 0, 6, 5, 5, 1, 6, 7, 6, 1, 3, 4, 0, 9, 9, 6, 8, 8, 3, 2, 3, 3, 8, 5, 3, 0, 9, 0, 8, 1, 3, 4, 8, 2, 4, 6, 1, 3, 5, 5, 1, 1, 1, 9, 5, 0, 4, 2, 9, 2, 5, 7, 4, 3, 3, 9, 1, 7, 2, 6, 1, 2, 9, 7, 9, 0, 4, 7, 8, 4, 9, 9, 4, 2, 9, 9, 8, 8, 6, 3, 2, 4, 6, 1, 7, 8, 2, 5, 8, 0, 4, 4, 5, 2, 4, 6, 5, 6, 5, 7, 7, 4, 5, 2, 0, 1, 1, 9, 6, 4, 3, 8, 7, 4, 0, 1, 5, 5, 0, 7, 0, 8, 5, 6, 1, 2, 3, 5, 8, 9, 6, 7, 6, 0, 7, 3, 1, 9, 4, 1, 8, 8, 1, 0, 6, 1, 7, 2, 5, 4, 5, 6, 6, 4, 8, 6, 4, 7, 9, 9, 4, 5, 1, 3, 9, 8, 6, 7, 3, 9, 5, 2, 5, 2, 1, 7, 1, 7, 0, 8, 3, 8, 3, 4, 1, 4, 0, 7, 9, 8, 5, 5, 6, 3, 9, 1, 2, 0, 0, 2, 8, 0, 2, 9, 3, 2, 0, 9, 9, 3, 2, 0, 6, 9, 7, 3, 7, 5, 1, 4, 6, 0, 7, 5, 8, 6, 7, 5, 2, 5, 5, 3, 3, 2, 2, 9, 9, 8, 8, 3, 5, 4, 3, 2, 8, 1, 1, 0, 7, 2, 1, 8, 7, 7, 5, 7, 6, 0, 4, 0, 3, 7, 3, 6, 2, 6, 5, 6, 3, 0, 3, 4, 5, 8, 4, 4, 0, 0, 1, 0, 6, 3, 8, 0, 1, 0, 7, 1, 3, 1, 8, 0, 2, 9, 8, 6, 5, 8, 4, 3, 4, 1, 0, 3, 9, 7, 4, 0, 3, 0, 8, 1, 1, 5, 7, 0, 0, 4, 8, 8, 8, 4, 1, 2, 9, 1, 2, 8, 7, 0, 7, 6, 2, 9, 0, 6, 7, 7, 5, 8, 2, 4, 4, 8, 4, 8, 0, 2, 7, 7, 6, 9, 1, 9, 9, 7, 5, 3, 7, 6, 0, 3, 9, 2, 9, 5, 6, 1, 0, 0, 2, 4, 5, 6, 3, 6, 4, 3, 5, 8, 9, 3, 9, 3, 4, 9, 3, 9, 5, 2, 8, 2, 6, 2, 2, 7, 2, 7, 6, 2, 4, 1, 2, 9, 9, 5, 2, 9, 8, 7, 7, 5, 6, 2, 8, 3, 7, 2, 3, 7, 3, 3, 6, 2, 1, 1, 1, 6, 9, 4, 0, 2, 4, 8, 0, 5, 4, 3, 1, 6, 1, 9, 8, 1, 1, 2, 8, 4, 8, 8, 3, 2, 6, 1, 0, 8, 1, 7, 6, 8, 5, 6, 3, 6, 7, 9, 2, 7, 7, 0, 7, 1, 7, 0, 6, 2, 7, 5, 0, 6, 9, 6, 2, 3, 1, 4, 1, 6, 9, 0, 5, 8, 9, 9, 6, 0, 8, 2, 0, 8, 9, 9, 8, 4, 3, 3, 6, 7, 0, 6, 4, 9, 1, 5, 8, 0, 0, 1, 0, 8, 9, 3, 8, 9, 3, 7, 8, 9, 4, 4, 6, 0, 9, 7, 2, 8, 0, 6, 2, 0, 6, 5, 5, 8, 5, 8, 6, 4, 0, 2, 6, 4, 3, 6, 2, 3, 7, 7, 2, 4, 2, 4, 9, 0, 2, 5, 4, 5, 4, 6, 0, 7, 7, 5, 0, 2, 0, 8, 5, 3, 7, 4, 5, 1, 9, 6, 5, 1, 8, 5, 4, 8, 5, 2, 3, 3, 8, 2, 2, 8, 2, 5, 7, 5, 5, 8, 7, 6, 1, 1, 8, 5, 1, 9, 9, 2, 2, 9, 1, 5, 2, 1, 3, 9, 9, 1, 3, 1, 2, 0, 6, 1, 7, 6, 8, 4, 0, 3, 3, 7, 2, 3, 6, 4, 5, 2, 7, 2, 4, 9, 5, 1, 6, 8, 8, 2, 1, 7, 1, 8, 0, 8, 5, 7, 0, 3, 6, 4, 2, 7, 7, 8, 2, 8, 9, 8, 8, 4, 6, 7, 0, 1, 2, 1, 1, 9, 4, 6, 4, 2, 7, 4, 6, 1, 9, 4, 9, 1, 4, 5, 1, 9, 2, 8, 0, 1, 5, 1, 7, 9, 2, 0, 4, 8, 4, 0, 2, 0, 1, 0, 5, 3, 6, 2, 2, 2, 2, 6, 6, 8, 6, 2, 3, 5, 2, 8, 9, 8, 0, 1, 5, 0, 9, 1, 6, 7, 9, 7, 3, 6, 2, 0, 4, 7, 6, 4, 4, 5, 2, 3, 4, 4, 5, 2, 9, 6, 1, 4, 7, 4, 8, 6, 5, 9, 8, 9, 9, 4, 4, 8, 6, 6, 4, 2, 1, 8, 3, 3, 0, 3, 8, 9, 2, 6, 2, 6, 5, 6, 8, 4, 3, 5, 0, 3, 2, 0, 0, 4, 8, 9, 8, 6, 9, 2, 1, 0, 7, 9, 0, 5, 0, 6, 2, 0, 6, 7, 6, 2, 6, 7, 8, 2, 4, 7, 6, 8, 7, 7, 3, 2, 5, 9, 3, 8, 2, 6, 4, 9, 6, 2, 1, 4, 0, 6, 6, 4, 3, 4, 5, 3, 6, 5, 9, 9, 5, 1, 6, 0, 2, 7, 9, 5, 3, 5, 6, 2, 1, 5, 5, 9, 0, 1, 3, 4, 5, 7, 4, 5, 9, 6, 1, 7, 4, 2, 8, 8, 5, 6, 0, 3, 4, 9, 4, 5, 6, 0, 7, 0, 3, 0, 7, 5, 8, 3, 8, 1, 5, 9, 1, 0, 1, 8, 0, 8, 2, 5, 6, 7, 1, 0, 2, 9, 0, 4, 1, 9, 5, 3, 5, 7, 3, 1, 9, 3, 9, 4, 9, 6, 0, 8, 8, 6, 6, 9, 6, 9, 3, 2, 0, 1, 7, 3, 6, 5, 3, 3, 3, 6, 4, 0, 0, 8, 4, 2, 2, 2, 1, 8, 6, 8, 5, 3, 9, 9, 0, 9, 8, 1, 5, 4, 3, 7, 0, 9, 7, 6, 0, 4, 1, 9, 8, 4, 7, 7, 5, 9, 7, 7, 5, 5, 6, 8, 5, 7, 5, 8, 8, 2, 5, 5, 2, 8, 4, 5, 6, 8, 8, 5, 3, 5, 5, 9, 4, 3, 3, 9, 0, 2, 4, 1, 0, 2, 7, 5, 3, 4, 3, 7, 6, 2, 3, 5, 2, 0, 5, 9, 6, 3, 9, 6, 0, 2, 5, 6, 7, 7, 2, 0, 1, 7, 0, 8, 9, 4, 3, 4, 3, 5, 3, 6, 0, 4, 2, 6, 9, 5, 1, 1, 9, 1, 9, 2, 4, 9, 1, 9, 8, 6, 3, 6, 8, 2, 0, 9, 3, 5, 8, 7, 4, 2, 2, 7, 9, 1, 8, 2, 4, 7, 6, 0, 5, 5, 6, 9, 8, 8, 5, 5, 7, 1, 2, 7, 9, 0, 5, 7, 5, 2, 5, 4, 7, 1, 3, 3, 4, 3, 6, 2, 0, 5, 8, 0, 4, 2, 9, 2, 4, 2, 3, 8, 0, 8, 4, 6, 4, 2, 8, 6, 9, 8, 4, 1, 9, 7, 0, 8, 3, 2, 4, 6, 8, 2, 4, 1, 3, 0, 8, 2, 0, 8, 0, 0, 0, 0, 2, 2, 3, 6, 9, 7, 0, 9, 1, 0, 7, 2, 8, 3, 0, 8, 4, 3, 5, 8, 7, 8, 4, 0, 0, 9, 7, 3, 1, 5, 2, 6, 5, 3, 2, 3, 7, 0, 3, 4, 1, 1, 8, 9, 0, 4, 8, 3, 1, 1, 5, 7, 2, 3, 9, 4, 5, 2, 5, 4, 7, 1, 7, 6, 2, 6, 1, 8, 8, 9, 4, 9, 5, 6, 6, 7, 1, 1, 2, 2, 4, 8, 5, 5, 9, 2, 9, 8, 5, 7, 2, 3, 8, 5, 7, 4, 7, 6, 2, 6, 0, 6, 2, 0, 4, 0, 1, 4, 2, 3, 5, 8, 7, 5, 1, 5, 2, 2, 5, 3, 6, 5, 4, 3, 2, 9, 4, 4, 0, 1, 8, 8, 6, 5, 3, 7, 2, 3, 2, 9, 0, 9, 2, 7, 8, 1, 2, 1, 3, 7, 2, 9, 3, 5, 5, 8, 4, 5, 5, 8, 5, 5, 6, 9, 5, 8, 6, 1, 2, 4, 2, 4, 9, 2, 1, 9, 3, 0, 8, 2, 8, 4, 2, 6, 4, 1, 3, 6, 9, 4, 6, 8, 6, 9, 6, 4, 7, 2, 0, 9, 4, 4, 4, 8, 9, 7, 7, 9, 3, 5, 0, 0, 3, 3, 3, 7, 6, 1, 4, 5, 2, 4, 4, 8, 5, 6, 0, 3, 4, 6, 3, 2, 0, 3, 5, 7, 0, 3, 5, 4, 7, 8, 5, 7, 0, 2, 8, 0, 2, 4, 6, 6, 6, 3, 7, 6, 3, 5, 9, 9, 7, 4, 9, 1, 9, 6, 5, 1, 7, 0, 2, 0, 2, 7, 4, 9, 8, 3, 5, 3, 0, 5, 1, 6, 7, 7, 8, 8, 2, 7, 9, 2, 4, 6, 5, 9, 8, 4, 6, 7, 8, 2, 8, 5, 8, 9, 0, 2, 3, 0, 9, 7, 3, 7, 9, 5, 3, 6, 6, 9, 9, 1, 5, 1, 2, 2, 4, 0, 9, 5, 4, 2, 8, 9, 9, 2, 0, 2, 6, 9, 1, 3, 8, 8, 6, 6, 4, 9, 6, 4, 4, 0, 4, 9, 2, 2, 0, 4, 0, 9, 6, 7, 9, 0, 9, 3, 9, 9, 9, 6, 2, 3, 6, 7, 6, 0, 5, 7, 6, 4, 4, 9, 8, 0, 9, 4, 6, 0, 7, 1, 3, 6, 6, 0, 4, 7, 1, 1, 4, 9, 8, 6, 8, 0, 6, 1, 1, 8, 5, 5, 5, 3, 4, 5, 5, 8, 6, 9, 3, 4, 3, 4, 2, 4, 3, 3, 8, 8, 9, 5, 7, 2, 8, 2, 3, 1, 9, 6, 4, 8, 5, 1, 6, 1, 7, 6, 4, 3, 2, 3, 9, 0, 5, 2, 3, 9, 5, 3, 2, 4, 4, 5, 8, 3, 4, 5, 6, 6, 7, 3, 1, 6, 5, 9, 8, 3, 2, 0, 4, 7, 3, 9, 9, 4, 1, 6, 7, 6, 6, 1, 5, 8, 1, 0, 8, 6, 3, 7, 1, 5, 9, 4, 1, 6, 0, 3, 7, 2, 9, 2, 2, 6, 9, 1, 0, 5, 7, 4, 2, 8, 9, 0, 8, 6, 4, 3, 3, 6, 4, 5, 4, 2, 3, 1, 4, 6, 8, 3, 9, 7, 9, 4, 0, 8, 9, 8, 0, 8, 1, 9, 3, 2, 7, 0, 6, 0, 5, 9, 5, 4, 8, 1, 0, 2, 5, 0, 8, 3, 0, 1, 4, 0, 3, 1, 6, 4, 7, 9, 5, 5, 0, 4, 7, 9, 1, 9, 1, 4, 6, 3, 9, 9, 9, 8, 9, 7, 1, 1, 4, 1, 0, 7, 0, 9, 6, 6, 2, 4, 7, 6, 4, 5, 8, 0, 4, 7, 1, 7, 3, 4, 7, 5, 5, 2, 0, 2, 0, 5, 5, 8, 5, 8, 4, 0, 8, 4, 9, 7, 6, 9, 8, 0, 8, 4, 2, 9, 1, 4, 6, 0, 7, 7, 1, 6, 1, 5, 6, 5, 7, 3, 8, 4, 5, 8, 5, 8, 5, 9, 1, 3, 2, 5, 3, 1, 2, 1, 2, 6, 7, 2, 0, 5, 3, 1, 6, 2, 7, 3, 6, 0, 9, 8, 0, 6, 4, 3, 2, 6, 8, 7, 2, 1, 1, 9, 5, 4, 5, 8, 0, 1, 0, 9, 1, 8, 2, 4, 7, 4, 2, 4, 4, 7, 0, 0, 6, 4, 6, 7, 8, 5, 9, 9, 2, 9, 3, 3, 5, 7, 7, 6, 7, 0, 0, 0, 6, 3, 1, 2, 6, 3, 7, 3, 2, 2, 8, 8, 6, 6, 0, 0, 3, 1, 1, 1, 3, 4, 4, 6, 4, 7, 3, 2, 3, 9, 7, 2, 7, 9, 2, 3, 6, 7, 5, 9, 2, 6, 3, 5, 6, 5, 3, 4, 6, 9, 1, 7, 7, 5, 9, 4, 2, 3, 8, 0, 9, 6, 6, 1, 3, 6, 6, 3, 9, 3, 6, 5, 2, 0, 4, 7, 4, 8, 5, 5, 0, 0, 9, 0, 3, 6, 2, 9, 8, 1, 1, 4, 3, 4, 0, 3, 7, 3, 8, 6, 4, 2, 4, 1, 2, 7, 5, 2, 6, 8, 0, 4, 6, 4, 2, 3, 2, 4, 1, 6, 0, 5, 8, 8, 1, 8, 6, 2, 8, 2, 4, 4, 0, 9, 7, 6, 7, 4, 2, 3, 8, 9, 4, 8, 8, 4, 2, 0, 1, 4, 4, 8, 6, 6, 7, 2, 7, 0, 0, 7, 2, 3, 1, 8, 9, 7, 7, 0, 1, 7, 9, 7, 7, 5, 9, 2, 7, 5, 5, 1, 4, 4, 7, 1, 3, 6, 3, 0, 0, 0, 8, 0, 8, 5, 6, 8, 6, 6, 4, 0, 0, 8, 6, 7, 4, 8, 3, 9, 4, 9, 8, 2, 3, 0, 4, 4, 8, 1, 8, 4, 3, 8, 8, 4, 2, 4, 3, 6, 3, 0, 5, 8, 0, 7, 5, 4, 7, 5, 8, 9, 0, 4, 6, 7, 1, 1, 4, 3, 5, 7, 1, 7, 1, 9, 3, 0, 0, 8, 9, 7, 2, 8, 9, 9, 3, 5, 0, 3, 3, 2, 1, 0, 1, 5, 2, 7, 1, 4, 1, 3, 5, 1, 9, 8, 5, 2, 0, 7, 2, 9, 2, 0, 9, 2, 2, 6, 2, 1, 8, 4, 4, 8, 4, 1, 7, 0, 3, 6, 1, 6, 3, 7, 4, 4, 7, 8, 2, 7, 4, 7, 3, 6, 6, 3, 5, 9, 8, 6, 0, 0, 8, 4, 3, 6, 9, 6, 2, 5, 0, 6, 3, 1, 5, 2, 2, 7, 3, 4, 6, 2, 8, 7, 6, 3, 9, 3, 4, 3, 6, 7, 7, 1, 1, 7, 8, 4, 5, 3, 6, 5, 6, 2, 9, 2, 5, 9, 1, 1, 7, 8, 3, 5, 1, 3, 8, 5, 0, 8, 0, 2, 3, 9, 8, 3, 0, 0, 7, 5, 7, 3, 2, 3, 3, 7, 5, 5, 6, 9, 0, 2, 1, 2, 5, 5, 1, 4, 0, 7, 1, 8, 8, 4, 0, 1, 7, 8, 9, 0, 7, 2, 5, 7, 8, 3, 0, 1, 2, 3, 9, 2, 7, 0, 0, 2, 2, 4, 1, 5, 3, 3, 5, 1, 5, 5, 9, 0, 9, 8, 0, 9, 3, 0, 7, 9, 6, 5, 5, 2, 6, 8, 0, 1, 7, 7, 7, 1, 3, 0, 1, 4, 1, 0, 3, 1, 9, 0, 6, 3, 6, 0, 5, 4, 0, 8, 8, 0, 1, 5, 8, 3, 7, 2, 9, 1, 9, 9, 2, 7, 6, 1, 0, 9, 8, 8, 6, 0, 2, 0, 5, 0, 8, 9, 2, 6, 6, 1, 2, 1, 5, 8, 9, 7, 4, 1, 2, 8, 9, 0, 2, 0, 9, 9, 1, 1, 5, 5, 3, 9, 3, 1, 5, 2, 2, 5, 1, 6, 9, 5, 4, 5, 4, 9, 6, 0, 8, 2, 1, 9, 7, 0, 8, 0, 1, 0, 2, 3, 8, 6, 4, 1, 5, 6, 1, 9, 4, 9, 6, 3, 8, 7, 3, 0, 3, 9, 7, 2, 6, 2, 9, 4, 6, 0, 9, 6, 7, 4, 7, 0, 9, 9, 5, 6, 5, 4, 1, 3, 5, 5, 2, 9, 5, 2, 9, 7, 7, 4, 9, 4, 4, 8, 7, 8, 7, 4, 5, 0, 8, 2, 3, 0, 0, 8, 8, 4, 0, 2, 5, 7, 2, 6, 2, 5, 6, 9, 7, 3, 4, 1, 5, 4, 8, 8, 4, 3, 0, 7, 2, 9, 4, 6, 6, 4, 9, 2, 4, 8, 8, 0, 3, 8, 1, 4, 1, 0, 6, 6, 9, 1, 6, 2, 3, 4, 0, 1, 2, 2, 0, 8, 8, 4, 7, 9, 9, 6, 6, 0, 5, 6, 7, 3, 6, 7, 8, 7, 3, 1, 0, 3, 8, 4, 5, 1, 8, 6, 2, 8, 5, 7, 6, 2, 8, 6, 5, 8, 0, 5, 9, 2, 7, 0, 5, 8, 6, 8, 9, 3, 8, 3, 2, 6, 8, 7, 8, 3, 3, 8, 5, 4, 1, 2, 9, 6, 5, 9, 4, 3, 8, 5, 5, 8, 8, 2, 0, 3, 1, 4, 1, 0, 8, 0, 5, 8, 4, 9, 1, 8, 0, 0, 6, 7, 7, 7, 5, 6, 3, 6, 5, 8, 2, 6, 5, 7, 5, 4, 7, 3, 2, 9, 5, 8, 8, 5, 7, 2, 2, 8, 7, 6, 7, 7, 2, 7, 6, 4, 0, 6, 3, 1, 4, 6, 3, 9, 8, 3, 8, 1, 0, 0, 5, 3, 5, 8, 4, 0, 6, 1, 7, 0, 2, 2, 1, 7, 3, 0, 8, 6, 7, 7, 0, 0, 1, 4, 1, 1, 6, 6, 6, 1, 3, 6, 0, 1, 3, 8, 5, 0, 1, 6, 5, 5, 4, 3, 9, 8, 6, 0, 5, 4, 9, 2, 8, 4, 8, 8, 2, 1, 4, 8, 6, 7, 3, 1, 3, 4, 9, 4, 8, 4, 5, 0, 9, 1, 3, 8, 7, 5, 4, 6, 6, 7, 9, 0, 5, 2, 3, 3, 3, 9, 0, 9, 2, 9, 1, 0, 2, 3, 9, 6, 6, 1, 6, 3, 7, 6, 2, 7, 0, 4, 8, 9, 6, 0, 7, 7, 5, 0, 4, 5, 9, 6, 4, 5, 7, 9, 3, 7, 8, 5, 0, 4, 1, 3, 8, 6, 9, 5, 7, 6, 7, 7, 3, 4, 2, 3, 6, 4, 5, 0, 1, 9, 3, 8, 4, 4, 1, 9, 8, 9, 3, 6, 0, 9, 1, 6, 3, 2, 3, 0, 5, 8, 8, 0, 0, 6, 2, 3, 5, 1, 5, 7, 1, 0, 9, 8, 3, 3, 5, 6, 3, 2, 1, 3, 7, 0, 7, 7, 2, 4, 7, 7, 6, 2, 6, 1, 7, 2, 3, 3, 0, 2, 2, 9, 6, 5, 5, 1, 9, 7, 9, 4, 6, 4, 1, 9, 1, 3, 6, 1, 2, 2, 5, 8, 0, 8, 5, 1, 7, 6, 7, 9, 3, 0, 9, 2, 2, 6, 7, 6, 9, 1, 9, 4, 0, 5, 6, 6, 4, 4, 4, 1, 6, 3, 2, 8, 9, 4, 7, 2, 3, 9, 3, 4, 0, 1, 1, 1, 9, 9, 7, 5, 1, 9, 2, 5, 6, 1, 5, 3, 0, 1, 4, 9, 4, 2, 0, 8, 3, 2, 7, 3, 8, 6, 1, 4, 2, 4, 9, 4, 6, 9, 1, 5, 9, 9, 1, 4, 2, 7, 2, 7, 3, 1, 9, 8, 8, 1, 2, 0, 8, 8, 2, 8, 7, 8, 4, 1, 7, 4, 3, 1, 8, 6, 4, 5, 0, 5, 6, 7, 5, 9, 8, 8, 8, 9, 7, 3, 8, 7, 8, 2, 1, 8, 8, 4, 9, 1, 5, 4, 2, 6, 6, 9, 2, 7, 2, 8, 4, 2, 9, 1, 6, 5, 8, 5, 9, 7, 6, 2, 9, 1, 8, 7, 0, 9, 9, 7, 6, 3, 9, 3, 0, 0, 0, 7, 7, 0, 7, 3, 9, 0, 9, 2, 1, 5, 7, 1, 6, 8, 2, 0, 4, 2, 1, 4, 0, 4, 8, 0, 4, 9, 9, 6, 3, 6, 1, 7, 4, 6, 8, 6, 0, 7, 4, 8, 7, 5, 7, 5, 2, 0, 5, 8, 6, 6, 6, 8, 8, 5, 6, 6, 9, 9, 2, 7, 2, 1, 9, 3, 9, 0, 1, 5, 9, 9, 7, 9, 6, 5, 3, 7, 6, 7, 0, 4, 2, 3, 9, 6, 2, 5, 7, 5, 0, 2, 8, 2, 9, 6, 1, 5, 1, 3, 6, 3, 0, 8, 0, 6, 3, 5, 0, 1, 4, 6, 7, 9, 1, 5, 4, 0, 4, 6, 2, 3, 8, 7, 5, 7, 3, 4, 8, 9, 8, 3, 6, 0, 5, 1, 1, 9, 4, 8, 9, 9, 9, 5, 0, 8, 0, 2, 0, 3, 6, 3, 9, 0, 3, 3, 8, 0, 8, 8, 0, 9, 4, 8, 4, 6, 4, 4, 3, 3, 0, 4, 9, 6, 7, 9, 5, 4, 3, 5, 5, 5, 2, 3, 6, 5, 5, 9, 1, 6, 4, 1, 0, 5, 7, 9, 3, 0, 6, 8, 9, 7, 6, 2, 8, 0, 0, 2, 9, 0, 3, 2, 7, 2, 7, 3, 2, 3, 7, 7, 5, 3, 8, 5, 4, 0, 2, 9, 1, 3, 2, 5, 8, 0, 6, 1, 7, 5, 9, 7, 7, 5, 0, 1, 1, 9, 4, 2, 1, 8, 2, 2, 2, 7, 3, 7, 0, 4, 6, 6, 2, 9, 3, 0, 6, 0, 1, 9, 3, 5, 4, 9, 6, 7, 3, 6, 8, 3, 9, 2, 0, 9, 5, 4, 7, 2, 3, 9, 9, 5, 0, 3, 1, 7, 2, 4, 7, 9, 9, 0, 4, 4, 5, 1, 7, 1, 8, 4, 1, 4, 2, 1, 6, 9, 2, 2, 0, 8, 0, 8, 8, 5, 5, 2, 8, 0, 9, 5, 7, 4, 9, 5, 2, 0, 8, 0, 2, 6, 9, 1, 3, 0, 1, 8, 9, 4, 5, 9, 8, 0, 7, 8, 4, 4, 3, 6, 6, 5, 6, 1, 0, 1, 2, 8, 9, 6, 5, 9, 9, 9, 6, 5, 0, 9, 5, 0, 5, 8, 0, 0, 4, 3, 4, 2, 6, 1, 7, 6, 2, 3, 1, 2, 8, 2, 6, 0, 3, 4, 1, 6, 2, 3, 4, 2, 6, 5, 8, 9, 8, 3, 5, 7, 5, 7, 0, 4, 6, 3, 4, 0, 3, 6, 2, 1, 7, 5, 8, 5, 8, 7, 4, 5, 4, 0, 2, 1, 5, 7, 2, 6, 8, 0, 7, 7, 4, 1, 0, 7, 9, 2, 7, 0, 9, 8, 7, 7, 5, 2, 7, 9, 7, 6, 4, 8, 4, 6, 4, 1, 7, 9, 8, 8, 6, 3, 7, 3, 3, 9, 7, 0, 7, 1, 8, 8, 5, 7, 1, 1, 7, 4, 0, 4, 1, 2, 2, 9, 8, 7, 1, 3, 2, 2, 4, 1, 0, 4, 5, 1, 5, 6, 1, 8, 2, 5, 3, 9, 5, 1, 7, 0, 4, 6, 7, 2, 3, 5, 5, 9, 0, 5, 7, 8, 3, 6, 0, 3, 1, 9, 2, 7, 0, 9, 8, 2, 9, 8, 3, 2, 5, 0, 0, 8, 3, 3, 6, 6, 4, 9, 6, 0, 9, 1, 5, 7, 0, 2, 7, 8, 2, 9, 4, 1, 3, 1, 6, 5, 6, 2, 4, 2, 8, 5, 8, 6, 4, 4, 8, 8, 6, 2, 3, 3, 8, 1, 3, 4, 5, 6, 2, 9, 0, 7, 2, 7, 0, 8, 7, 4, 8, 2, 7, 7, 7, 9, 9, 1, 4, 8, 3, 7, 4, 3, 4, 1, 8, 8, 4, 1, 0, 8, 0, 0, 0, 4, 7, 1, 5, 5, 5, 3, 6, 5, 3, 2, 9, 0, 5, 1, 0, 0, 9, 1, 7, 8, 9, 2, 3, 5, 7, 1, 0, 8, 8, 8, 0, 7, 2, 7, 2, 5, 0, 7, 7, 9, 4, 8, 6, 5, 9, 6, 8, 6, 9, 4, 0, 4, 7, 2, 9, 0, 3, 0, 9, 5, 2, 2, 1, 4, 4, 2, 5, 6, 2, 4, 2, 5, 3, 1, 7, 5, 9, 6, 9, 5, 9, 4, 1, 2, 5, 9, 8, 3, 6, 5, 5, 4, 8, 7, 9, 5, 0, 8, 4, 9, 5, 7, 3, 3, 0, 2, 1, 5, 9, 0, 1, 2, 3, 3, 7, 2, 1, 0, 7, 7, 1, 3, 4, 2, 5, 9, 4, 0, 8, 9, 5, 3, 0, 5, 5, 6, 5, 6, 6, 2, 0, 8, 5, 6, 5, 4, 5, 5, 2, 6, 3, 1, 4, 9, 0, 0, 9, 4, 9, 7, 6, 7, 1, 1, 1, 8, 0, 6, 2, 6, 4, 9, 1, 5, 0, 2, 8, 3, 1, 1, 8, 8, 5, 1, 0, 8, 4, 6, 2, 7, 6, 9, 9, 3, 9, 8, 5, 5, 2, 7, 9, 8, 8, 1, 3, 9, 7, 0, 8, 3, 4, 3, 0, 9, 6, 4, 9, 4, 8, 2, 9, 6, 4, 4, 8, 9, 5, 3, 3, 0, 0, 7, 0, 1, 8, 6, 7, 9, 1, 7, 3, 3, 3, 9, 5, 4, 4, 3, 4, 3, 3, 2, 2, 4, 4, 6, 4, 5, 7, 3, 7, 8, 5, 0, 5, 3, 6, 8, 1, 2, 2, 3, 2, 1, 5, 1, 4, 7, 2, 9, 8, 7, 0, 1, 0, 4, 1, 9, 6, 8, 5, 4, 8, 4, 6, 5, 6, 7, 3, 7, 9, 5, 0, 7, 6, 3, 9, 2, 4, 6, 9, 1, 4, 7, 1, 6, 4, 9, 0, 6, 7, 5, 4, 4, 9, 7, 3, 1, 9, 7, 9, 4, 6, 7, 4, 9, 6, 1, 2, 6, 4, 7, 6, 7, 1, 4, 3, 2, 3, 7, 0, 3, 9, 4, 0, 0, 4, 2, 8, 2, 1, 3, 7, 7, 8, 2, 3, 1, 7, 2, 0, 7, 3, 4, 3, 5, 2, 7, 6, 8, 5, 4, 5, 2, 8, 3, 4, 5, 8, 1, 3, 1, 6, 3, 5, 1, 0, 5, 1, 9, 5, 2, 9, 4, 2, 7, 9, 9, 1, 6, 3, 2, 0, 1, 7, 0, 0, 1, 0, 5, 4, 8, 0, 9, 0, 1, 9, 3, 7, 4, 2, 4, 2, 6, 2, 0, 3, 6, 0, 8, 7, 0, 0, 7, 3, 8, 3, 3, 6, 8, 2, 7, 4, 1, 2, 9, 2, 4, 6, 6, 8, 5, 1, 4, 3, 7, 5, 0, 9, 2, 0, 1, 3, 9, 4, 3, 1, 8, 5, 4, 9, 9, 0, 7, 6, 5, 5, 2, 1, 6, 2, 7, 4, 6, 4, 1, 6, 5, 1, 9, 3, 8, 7, 2, 3, 6, 1, 4, 3, 2, 8, 2, 8, 6, 4, 6, 8, 4, 5, 2, 8, 0, 9, 7, 3, 5, 2, 4, 6, 7, 2, 1, 3, 3, 3, 3, 1, 1, 3, 7, 4, 2, 6, 8, 2, 0, 0, 4, 6, 3, 8, 4, 6, 3, 0, 7, 8, 1, 9, 6, 0, 5, 2, 6, 7, 6, 4, 9, 6, 4, 7, 8, 5, 9, 4, 5, 1, 4, 6, 6, 7, 9, 3, 8, 2, 0, 1, 6, 9, 2, 6, 2, 1, 6, 7, 0, 6, 3, 3, 3, 2, 1, 4, 5, 4, 0, 7, 8, 6, 3, 7, 4, 0, 9, 6, 7, 3, 5, 8, 5, 6, 9, 5, 6, 6, 2, 3, 3, 4, 9, 7, 1, 3, 4, 6, 2, 4, 3, 3, 0, 8, 6, 7, 7, 3, 4, 5, 4, 2, 3, 1, 0, 9, 4, 4, 4, 2, 5, 2, 0, 3, 3, 6, 2, 5, 1, 7, 9, 2, 8, 2, 0, 1, 8, 8, 7, 2, 3, 4, 3, 2, 1, 0, 7, 5, 9, 9, 6, 8, 6, 2, 3, 0, 8, 4, 7, 4, 7, 1, 9, 3, 1, 2, 1, 9, 0, 6, 0, 9, 8, 1, 3, 1, 9, 6, 1, 3, 5, 7, 7, 1, 8, 7, 4, 0, 1, 3, 2, 6, 0, 0, 5, 3, 2, 9, 9, 5, 7, 7, 5, 5, 7, 3, 1, 0, 1, 2, 9, 9, 5, 9, 1, 4, 0, 6, 7, 3, 7, 3, 8, 8, 0, 4, 6, 6, 5, 3, 1, 7, 9, 8, 3, 5, 1, 3, 1, 7, 0, 1, 1, 2, 7, 8, 6, 3, 0, 2, 6, 9, 6, 7, 6, 6, 1, 7, 3, 6, 6, 8, 2, 3, 7, 5, 0, 8, 7, 7, 1, 9, 3, 6, 1, 4, 5, 2, 8, 2, 8, 7, 6, 5, 1, 4, 5, 3, 3, 9, 4, 1, 3, 8, 8, 3, 8, 7, 7, 5, 6, 8, 5, 1, 2, 7, 2, 2, 8, 6, 2, 8, 5, 7, 4, 6, 9, 3, 1, 1, 5, 2, 8, 7, 1, 1, 7, 7, 8, 6, 8, 6, 7, 1, 6, 8, 1, 6, 0, 9, 9, 2, 3, 4, 7, 4, 7, 7, 2, 4, 9, 8, 3, 8, 3, 7, 1, 4, 5, 6, 3, 2, 2, 0, 6, 0, 9, 8, 4, 7, 7, 5, 0, 3, 1, 9, 1, 3, 5, 5, 4, 7, 4, 5, 4, 8, 8, 8, 3, 2, 1, 2, 9, 6, 1, 9, 4, 7, 6, 4, 1, 1, 4, 8, 1, 9, 2, 3, 9, 0, 3, 9, 4, 1, 8, 3, 4, 8, 3, 3, 1, 8, 2, 6, 0, 0, 2, 1, 0, 4, 6, 6, 2, 1, 0, 3, 1, 6, 1, 9, 6, 3, 9, 6, 7, 4, 6, 8, 4, 2, 3, 2, 0, 8, 6, 4, 9, 0, 7, 0, 1, 2, 5, 5, 1, 6, 3, 6, 2, 8, 5, 7, 1, 6, 1, 2, 1, 0, 0, 8, 8, 2, 8, 5, 7, 3, 3, 5, 4, 9, 4, 7, 6, 1, 8, 4, 9, 7, 4, 3, 8, 2, 9, 7, 7, 5, 6, 0, 6, 3, 1, 3, 8, 2, 2, 9, 7, 8, 8, 0, 0, 7, 5, 1, 1, 6, 5, 8, 7, 4, 6, 2, 7, 7, 8, 7, 5, 5, 7, 6, 4, 6, 2, 2, 8, 9, 6, 9, 4, 6, 8, 8, 2, 1, 3, 3, 8, 4, 0, 4, 7, 2, 5, 4, 4, 0, 8, 4, 3, 6, 3, 8, 8, 2, 9, 6, 9, 5, 6, 2, 3, 7, 1, 4, 7, 9, 0, 5, 8, 6, 7, 3, 1, 7, 2, 5, 6, 6, 0, 5, 0, 8, 4, 2, 4, 4, 7, 1, 5, 8, 5, 2, 3, 8, 8, 4, 9, 8, 5, 9, 5, 2, 5, 8, 4, 7, 3, 3, 3, 1, 3, 8, 9, 3, 8, 1, 8, 3, 6, 9, 2, 2, 6, 3, 5, 5, 7, 3, 6, 0, 1, 2, 1, 1, 8, 5, 2, 7, 3, 4, 4, 8, 5, 5, 6, 5, 3, 5, 4, 5, 2, 2, 9, 0, 8, 9, 3, 6, 1, 0, 6, 1, 1, 2, 0, 3, 5, 7, 0, 2, 7, 7, 0, 4, 5, 6, 7, 5, 7, 2, 5, 0, 7, 1, 9, 3, 7, 6, 0, 7, 3, 4, 4, 5, 2, 9, 2, 0, 3, 1, 6, 2, 2, 4, 7, 3, 1, 8, 7, 3, 0, 6, 5, 3, 5, 3, 8, 6, 8, 3, 3, 5, 8, 1, 6, 9, 5, 6, 0, 9, 9, 5, 6, 8, 9, 6, 0, 6, 3, 6, 1, 0, 6, 2, 3, 5, 1, 9, 0, 6, 6, 5, 6, 3, 4, 5, 1, 6, 7, 4, 2, 9, 6, 6, 2, 5, 8, 8, 4, 1, 7, 1, 8, 1, 1, 3, 7, 9, 5, 8, 4, 4, 1, 2, 0, 1, 4, 9, 2, 7, 5, 8, 3, 2, 3, 8, 0, 1, 3, 7, 6, 6, 4, 3, 5, 0, 6, 4, 7, 1, 7, 1, 0, 0, 8, 2, 7, 2, 7, 8, 6, 2, 9, 1, 6, 2, 6, 1, 3, 0, 5, 3, 0, 6, 1, 1, 5, 1, 7, 5, 3, 4, 1, 1, 4, 0, 8, 4, 2, 5, 4, 8, 3, 4, 3, 1, 1, 7, 9, 1, 1, 8, 0, 1, 1, 2, 1, 4, 4, 8, 7, 4, 8, 6, 3, 1, 9, 6, 6, 4, 4, 0, 2, 5, 9, 7, 1, 7, 6, 4, 7, 5, 4, 1, 0, 2, 3, 3, 8, 9, 9, 7, 5, 9, 9, 7, 9, 6, 0, 4, 5, 3, 3, 1, 5, 1, 8, 3, 8, 0, 5, 4, 8, 1, 3, 8, 9, 1, 8, 0, 0, 5, 8, 1, 2, 7, 8, 2, 9, 5, 8, 9, 9, 6, 4, 3, 9, 0, 7, 0, 2, 2, 3, 8, 3, 0, 9, 5, 6, 3, 1, 5, 2, 3, 9, 2, 6, 7, 3, 8, 3, 6, 3, 7, 3, 0, 6, 3, 3, 9, 1, 8, 8, 3, 8, 3, 3, 0, 1, 0, 0, 7, 8, 8, 0, 5, 0, 9, 1, 8, 5, 6, 3, 5, 3, 0, 2, 2, 8, 6, 2, 9, 0, 0, 1, 0, 4, 3, 3, 8, 8, 4, 0, 6, 3, 1, 6, 2, 0, 7, 5, 7, 0, 1, 6, 0, 3, 2, 6, 9, 2, 9, 4, 7, 4, 5, 8, 5, 4, 7, 7, 3, 3, 7, 8, 9, 9, 1, 0, 4, 6, 2, 7, 5, 1, 8, 0, 7, 9, 3, 4, 2, 1, 0, 2, 5, 6, 0, 3, 9, 4, 2, 1, 7, 0, 1, 5, 4, 3, 7, 1, 7, 1, 9, 7, 8, 7, 0, 7, 8, 3, 2, 8, 7, 4, 5, 7, 1, 4, 7, 4, 4, 8, 9, 9, 8, 0, 6, 2, 4, 2, 4, 7, 6, 8, 1, 7, 4, 0, 2, 9, 8, 0, 6, 1, 6, 3, 9, 2, 0, 0, 4, 9, 2, 1, 0, 2, 2, 0, 2, 3, 7, 3, 8, 6, 4, 3, 7, 1, 0, 6, 4, 5, 8, 5, 3, 3, 7, 1, 7, 5, 7, 9, 2, 3, 4, 9, 9, 8, 9, 8, 2, 5, 9, 5, 0, 6, 8, 9, 9, 6, 1, 1, 9, 1, 6, 9, 4, 2, 2, 6, 8, 7, 1, 3, 6, 1, 1, 1, 5, 7, 5, 0, 1, 5, 8, 0, 6, 4, 3, 1, 7, 8, 1, 2, 1, 2, 5, 4, 8, 2, 9, 0, 8, 3, 5, 4, 9, 3, 6, 5, 7, 0, 7, 9, 9, 2, 6, 9, 9, 3, 2, 4, 3, 5, 0, 3, 0, 5, 1, 4, 5, 1, 1, 5, 5, 7, 0, 2, 0, 5, 9, 4, 0, 3, 8, 9, 0, 9, 8, 6, 1, 2, 7, 6, 7, 0, 3, 3, 3, 0, 6, 2, 0, 7, 0, 4, 3, 7, 3, 5, 7, 7, 4, 7, 6, 1, 4, 6, 6, 6, 0, 1, 4, 7, 7, 7, 1, 7, 8, 3, 7, 3, 9, 3, 2, 1, 0, 6, 2, 7, 5, 0, 6, 7, 2, 4, 3, 2, 9, 4, 9, 8, 2, 0, 6, 1, 1, 7, 5, 0, 6, 6, 2, 3, 5, 8, 3, 5, 7, 1, 8, 2, 2, 1, 4, 6, 2, 0, 1, 7, 8, 1, 2, 0, 2, 9, 9, 3, 7, 7, 4, 2, 0, 6, 8, 5, 9, 4, 9, 2, 3, 0, 5, 3, 4, 6, 1, 1, 0, 1, 6, 9, 6, 2, 9, 5, 7, 7, 3, 0, 5, 6, 3, 2, 0, 3, 3, 4, 8, 5, 1, 6, 5, 7, 7, 8, 2, 9, 9, 7, 3, 8, 2, 8, 6, 2, 2, 1, 4, 7, 4, 4, 6, 6, 9, 7, 2, 4, 6, 0, 8, 6, 6, 7, 0, 0, 0, 8, 2, 6, 6, 1, 5, 0, 9, 5, 6, 1, 4, 0, 7, 3, 1, 7, 9, 0, 1, 6, 6, 8, 8, 3, 0, 3, 3, 1, 6, 9, 4, 8, 8, 8, 6, 6, 2, 3, 9, 3, 8, 6, 2, 2, 0, 2, 5, 6, 8, 0, 2, 2, 8, 8, 3, 3, 3, 8, 2, 6, 6, 1, 7, 2, 7, 5, 5, 3, 8, 2, 0, 8, 4, 4, 8, 4, 5, 5, 7, 6, 3, 2, 3, 8, 1, 1, 6, 9, 0, 6, 7, 2, 4, 3, 4, 6, 8, 6, 0, 8, 8, 9, 7, 2, 7, 0, 4, 2, 3, 0, 6, 1, 0, 9, 3, 9, 1, 6, 2, 1, 1, 1, 0, 0, 0, 0, 2, 3, 0, 9, 3, 3, 7, 1, 6, 7, 9, 8, 2, 8, 4, 7, 0, 4, 0, 1, 8, 2, 3, 8, 1, 9, 9, 1, 6, 7, 1, 4, 0, 3, 5, 1, 0, 4, 8, 3, 5, 9, 7, 4, 2, 9, 9, 0, 2, 1, 8, 3, 9, 3, 5, 0, 5, 2, 3, 9, 0, 3, 3, 8, 3, 2, 2, 5, 8, 2, 6, 6, 2, 2, 1, 1, 7, 4, 2, 3, 8, 1, 2, 3, 4, 0, 2, 7, 5, 1, 0, 1, 2, 3, 6, 2, 5, 1, 5, 0, 0, 1, 7, 7, 5, 5, 6, 0, 4, 0, 3, 8, 6, 9, 8, 6, 1, 0, 2, 2, 6, 1, 8, 5, 3, 1, 9, 1, 1, 8, 5, 6, 1, 4, 2, 8, 4, 2, 7, 5, 5, 8, 0, 5, 1, 9, 3, 4, 8, 0, 8, 3, 5, 6, 4, 3, 6, 4, 0, 3, 4, 5, 3, 4, 2, 5, 3, 8, 0, 5, 8, 4, 0, 8, 5, 2, 5, 7, 8, 4, 2, 9, 5, 8, 8, 3, 3, 5, 3, 4, 9, 5, 0, 6, 1, 9, 0, 0, 9, 7, 1, 8, 3, 9, 2, 1, 7, 1, 9, 6, 9, 7, 7, 0, 2, 7, 7, 9, 9, 3, 5, 2, 2, 9, 2, 5, 5, 9, 5, 8, 3, 4, 1, 2, 5, 6, 3, 8, 4, 1, 2, 2, 0, 7, 6, 7, 1, 2, 8, 3, 1, 7, 4, 5, 9, 0, 3, 7, 8, 0, 7, 4, 2, 6, 9, 0, 2, 7, 4, 7, 3, 1, 7, 5, 1, 4, 0, 2, 4, 4, 3, 3, 0, 0, 8, 3, 3, 0, 9, 1, 7, 7, 0, 6, 9, 2, 4, 8, 5, 8, 5, 8, 9, 4, 4, 4, 8, 5, 3, 7, 0, 2, 5, 9, 8, 4, 9, 4, 2, 1, 3, 0, 3, 4, 8, 0, 1, 1, 9, 8, 7, 6, 1, 6, 3, 9, 6, 0, 2, 9, 2, 9, 6, 0, 0, 8, 6, 4, 9, 1, 1, 0, 5, 0, 2, 8, 4, 5, 9, 2, 6, 7, 4, 5, 5, 7, 6, 3, 8, 7, 9, 8, 4, 4, 6, 1, 1, 2, 0, 9, 1, 7, 2, 8, 8, 2, 1, 4, 4, 8, 4, 4, 6, 9, 5, 8, 5, 5, 7, 7, 5, 9, 5, 3, 4, 9, 9, 3, 7, 6, 9, 5, 8, 6, 6, 0, 7, 9, 2, 8, 3, 7, 0, 4, 0, 8, 8, 6, 2, 1, 2, 8, 9, 7, 2, 0, 2, 7, 6, 9, 1, 4, 8, 2, 8, 6, 7, 1, 1, 0, 1, 4, 6, 0, 4, 7, 1, 6, 0, 1, 9, 9, 8, 1, 6, 9, 1, 9, 9, 0, 1, 1, 7, 1, 8, 9, 1, 0, 0, 5, 7, 4, 4, 6, 5, 2, 5, 9, 8, 6, 5, 5, 3, 7, 8, 7, 7, 6, 3, 3, 1, 5, 9, 6, 7, 7, 3, 1, 7, 3, 2, 6, 6, 9, 8, 7, 2, 6, 8, 7, 7, 5, 1, 0, 7, 1, 7, 2, 1, 3, 4, 4, 9, 6, 2, 3, 8, 8, 0, 4, 7, 2, 7, 7, 9, 3, 7, 6, 8, 3, 9, 6, 4, 7, 4, 0, 3, 8, 9, 7, 0, 0, 5, 0, 7, 8, 8, 1, 8, 2, 9, 5, 8, 9, 5, 5, 2, 4, 8, 6, 3, 7, 2, 5, 5, 0, 3, 7, 4, 8, 1, 5, 1, 1, 7, 0, 1, 6, 7, 6, 8, 4, 0, 6, 6, 4, 2, 8, 7, 6, 3, 3, 7, 2, 3, 8, 9, 8, 7, 7, 4, 8, 3, 8, 7, 9, 2, 9, 9, 1, 5, 5, 4, 0, 6, 1, 2, 6, 3, 3, 2, 0, 5, 9, 7, 7, 0, 5, 0, 9, 2, 7, 7, 0, 9, 5, 5, 1, 9, 0, 3, 3, 9, 6, 7, 4, 8, 2, 5, 8, 2, 1, 4, 9, 0, 8, 2, 4, 0, 1, 2, 3, 0, 9, 4, 8, 3, 8, 9, 2, 3, 1, 4, 5, 1, 5, 1, 0, 2, 5, 2, 1, 8, 6, 4, 5, 0, 9, 3, 1, 0, 9, 1, 0, 7, 6, 6, 7, 7, 2, 6, 9, 3, 4, 5, 1, 9, 6, 6, 4, 8, 8, 6, 0, 4, 9, 5, 9, 9, 3, 9, 9, 0, 2, 2, 7, 3, 5, 1, 0, 9, 5, 8, 0, 8, 1, 1, 0, 0, 7, 4, 8, 2, 3, 3, 2, 5, 6, 0, 1, 8, 2, 6, 6, 2, 1, 3, 6, 5, 8, 3, 2, 0, 8, 1, 7, 9, 7, 2, 0, 4, 6, 8, 1, 5, 7, 9, 7, 0, 4, 1, 7, 5, 8, 7, 4, 8, 7, 3, 7, 4, 4, 0, 4, 7, 4, 3, 5, 3, 8, 1, 7, 8, 5, 7, 0, 8, 7, 5, 5, 4, 7, 1, 9, 0, 5, 2, 6, 1, 4, 8, 0, 6, 3, 1, 9, 7, 3, 5, 6, 6, 9, 8, 0, 5, 9, 6, 2, 6, 9, 5, 8, 3, 0, 5, 1, 7, 2, 8, 0, 4, 0, 8, 1, 3, 9, 7, 1, 3, 6, 7, 4, 8, 6, 7, 5, 3, 4, 3, 5, 2, 6, 9, 5, 0, 1, 7, 4, 8, 9, 4, 2, 2, 9, 0, 3, 3, 1, 3, 0, 4, 2, 1, 4, 0, 8, 2, 1, 4, 4, 3, 6, 6, 8, 3, 9, 9, 9, 9, 6, 5, 0, 8, 2, 3, 5, 9, 8, 2, 8, 3, 4, 1, 1, 7, 6, 9, 2, 1, 4, 6, 3, 3, 2, 1, 8, 1, 0, 0, 0, 3, 4, 0, 8, 0, 9, 1, 2, 8, 0, 1, 7, 7, 3, 4, 7, 8, 0, 2, 7, 1, 3, 7, 4, 6, 1, 7, 6, 1, 2, 6, 9, 8, 2, 0, 1, 0, 9, 6, 5, 3, 2, 4, 6, 5, 4, 9, 3, 8, 8, 7, 3, 6, 1, 2, 9, 8, 8, 1, 0, 3, 0, 8, 0, 0, 6, 6, 1, 3, 3, 6, 6, 5, 7, 5, 3, 8, 2, 1, 9, 0, 4, 6, 9, 2, 3, 9, 4, 1, 4, 2, 2, 3, 4, 8, 4, 3, 0, 6, 8, 8, 6, 2, 3, 3, 7, 3, 5, 2, 9, 6, 0, 2, 3, 5, 2, 4, 9, 5, 1, 6, 0, 3, 3, 7, 3, 7, 9, 8, 1, 5, 2, 8, 0, 2, 9, 5, 6, 6, 2, 6, 7, 4, 2, 4, 5, 8, 7, 6, 1, 1, 9, 5, 9, 7, 2, 1, 2, 9, 6, 8, 0, 3, 7, 7, 7, 8, 2, 3, 9, 0, 4, 1, 6, 4, 5, 0, 6, 6, 7, 2, 6, 9, 7, 9, 6, 6, 1, 3, 2, 4, 1, 1, 0, 4, 2, 6, 0, 8, 4, 4, 9, 8, 1, 5, 8, 1, 1, 8, 8, 9, 7, 2, 4, 5, 6, 4, 9, 6, 7, 5, 0, 8, 3, 5, 4, 5, 0, 5, 0, 2, 3, 0, 3, 7, 5, 0, 2, 3, 2, 3, 5, 9, 6, 3, 3, 8, 3, 9, 1, 7, 3, 0, 2, 0, 7, 5, 1, 1, 2, 6, 6, 1, 6, 8, 0, 8, 1, 0, 7, 2, 7, 3, 2, 3, 1, 9, 5, 3, 6, 9, 2, 4, 3, 8, 3, 8, 7, 2, 4, 2, 8, 4, 4, 2, 5, 6, 3, 9, 4, 6, 7, 0, 4, 4, 1, 8, 6, 2, 7, 4, 3, 1, 0, 7, 0, 9, 5, 8, 0, 1, 6, 9, 5, 0, 0, 2, 8, 5, 4, 0, 5, 1, 3, 3, 4, 9, 5, 3, 3, 1, 0, 1, 2, 2, 3, 6, 1, 2, 7, 8, 1, 9, 0, 6, 8, 6, 0, 8, 9, 2, 3, 7, 6, 2, 9, 8, 8, 6, 3, 0, 6, 9, 5, 9, 0, 7, 2, 0, 1, 2, 4, 1, 7, 5, 5, 2, 9, 2, 0, 5, 7, 4, 7, 7, 2, 7, 6, 5, 1, 5, 9, 7, 2, 0, 8, 9, 8, 6, 8, 5, 0, 7, 7, 8, 2, 1, 6, 1, 3, 1, 6, 6, 8, 2, 3, 7, 9, 8, 3, 8, 5, 7, 0, 1, 0, 7, 4, 2, 0, 4, 9, 4, 2, 7, 7, 4, 4, 1, 1, 0, 9, 0, 9, 8, 6, 4, 7, 2, 2, 2, 6, 0, 2, 1, 0, 2, 8, 4, 4, 6, 5, 9, 0, 1, 1, 8, 5, 1, 1, 8, 9, 0, 6, 4, 8, 4, 5, 7, 1, 4, 6, 2, 2, 6, 8, 2, 9, 2, 2, 7, 1, 4, 5, 3, 1, 0, 8, 4, 4, 3, 7, 8, 9, 3, 6, 1, 2, 1, 2, 2, 7, 1, 1, 1, 2, 5, 7, 2, 2, 7, 3, 4, 4, 5, 1, 0, 2, 9, 8, 3, 9, 1, 2, 0, 5, 9, 5, 9, 3, 1, 0, 3, 3, 0, 4, 0, 8, 4, 0, 1, 7, 5, 8, 3, 6, 9, 8, 7, 2, 8, 2, 3, 4, 5, 2, 6, 7, 4, 2, 7, 5, 5, 4, 8, 8, 1, 3, 8, 5, 4, 8, 9, 6, 9, 4, 3, 0, 1, 2, 2, 4, 6, 8, 8, 5, 0, 3, 1, 8, 3, 0, 7, 9, 8, 8, 9, 5, 2, 6, 4, 6, 8, 9, 8, 4, 8, 0, 5, 7, 1, 4, 8, 8, 3, 4, 0, 0, 5, 8, 3, 9, 3, 5, 9, 9, 6, 5, 8, 6, 3, 7, 2, 4, 0, 9, 2, 1, 0, 7, 3, 9, 8, 8, 4, 7, 7, 6, 9, 5, 8, 4, 6, 2, 2, 7, 6, 3, 3, 9, 2, 7, 6, 8, 1, 1, 5, 8, 5, 4, 4, 0, 3, 6, 1, 0, 6, 2, 6, 0, 3, 4, 6, 2, 6, 1, 1, 0, 7, 6, 5, 4, 5, 7, 1, 4, 4, 4, 7, 4, 4, 1, 4, 3, 6, 6, 8, 9, 6, 7, 2, 4, 3, 5, 3, 7, 8, 6, 2, 0, 0, 0, 2, 2, 5, 7, 1, 9, 6, 1, 9, 5, 9, 0, 6, 5, 4, 3, 8, 0, 6, 7, 1, 7, 4, 9, 8, 2, 6, 7, 3, 0, 5, 6, 7, 5, 7, 1, 6, 6, 4, 3, 3, 9, 8, 4, 3, 6, 4, 1, 6, 7, 4, 0, 1, 6, 3, 1, 2, 0, 0, 8, 6, 1, 6, 3, 8, 5, 0, 9, 1, 5, 4, 4, 0, 5, 2, 6, 1, 5, 9, 0, 8, 8, 1, 4, 9, 4, 4, 1, 0, 7, 3, 9, 1, 0, 2, 3, 1, 5, 2, 6, 9, 2, 3, 0, 7, 4, 3, 3, 0, 9, 3, 8, 3, 4, 2, 2, 1, 9, 0, 2, 8, 6, 6, 0, 3, 3, 6, 5, 3, 4, 1, 2, 0, 8, 9, 4, 1, 7, 2, 6, 1, 3, 3, 0, 1, 9, 5, 4, 4, 8, 2, 6, 2, 9, 7, 7, 7, 9, 8, 9, 4, 4, 7, 1, 0, 4, 3, 6, 3, 9, 8, 3, 6, 8, 3, 6, 6, 2, 6, 7, 3, 0, 0, 0, 2, 5, 1, 2, 9, 2, 2, 1, 6, 3, 9, 1, 1, 5]\n",
      "[[ 59  43  50 ... 140  84  72]\n",
      " [154 126 105 ... 139 142 144]\n",
      " [255 253 253 ...  83  83  84]\n",
      " ...\n",
      " [ 71  60  74 ...  68  69  68]\n",
      " [250 254 211 ... 215 255 254]\n",
      " [ 62  61  60 ... 130 130 131]]\n"
     ]
    }
   ],
   "source": [
    "# CHECKING DATA\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "dat = unpickle('cifar-10-batches-py/data_batch_1')[b'labels']\n",
    "data = unpickle('cifar-10-batches-py/data_batch_1')[b'data']\n",
    "\n",
    "key = unpickle('cifar-10-batches-py/data_batch_5').keys()\n",
    "print(dat)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6017f5aa-524c-4e31-b5e2-8aad3366eba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train loss 2.408974896916343, Test loss 2.310444891073142\n",
      "Epoch 1, Train loss 2.3071630534613528, Test loss 2.3076204782838277\n",
      "Epoch 2, Train loss 2.3069299938123855, Test loss 2.3081453924725768\n",
      "Epoch 3, Train loss 2.306243303181875, Test loss 2.306594730182818\n",
      "Epoch 4, Train loss 2.3065003708500385, Test loss 2.3049922946152415\n",
      "Epoch 5, Train loss 2.305988934338855, Test loss 2.3060046533110796\n",
      "Epoch 6, Train loss 2.3059512913379523, Test loss 2.3048667482509733\n",
      "Epoch 7, Train loss 2.3065944824682174, Test loss 2.3048215929869635\n",
      "Epoch 8, Train loss 2.3063857256603972, Test loss 2.306662518507356\n",
      "Epoch 9, Train loss 2.305964773268346, Test loss 2.306631789845266\n",
      "Epoch 10, Train loss 2.3064038402893963, Test loss 2.304871546994349\n",
      "Epoch 11, Train loss 2.3060644648569015, Test loss 2.304071364129425\n",
      "Epoch 12, Train loss 2.306389828167303, Test loss 2.3045431109750347\n",
      "Epoch 13, Train loss 2.3061651543278217, Test loss 2.304098223425021\n",
      "Epoch 14, Train loss 2.3056263835228923, Test loss 2.3095229537623703\n",
      "Epoch 15, Train loss 2.3065734580349737, Test loss 2.30717691949978\n",
      "Epoch 16, Train loss 2.3058980846648938, Test loss 2.3042653761092264\n",
      "Epoch 17, Train loss 2.3062952186750327, Test loss 2.3049857145661763\n",
      "Epoch 18, Train loss 2.306457774108633, Test loss 2.3043974600020487\n",
      "Epoch 19, Train loss 2.306306412762693, Test loss 2.3045003793801473\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "matches = 0\n",
    "total = 0\n",
    "\n",
    "# for epoch in range(3):\n",
    "#   train_loss = 0.0\n",
    "#   test_loss = 0.0\n",
    "\n",
    "#   model.train()\n",
    "#   for i, data in enumerate(zip(trainDataLoader, trainDataLoaderLabels)):\n",
    "#     images, labels = data\n",
    "#     images = images.cuda()\n",
    "#     labels = labels.cuda()\n",
    "#     optimizer.zero_grad() # zero out any gradient values from the previous iteration\n",
    "#     predicted_output = model(images) # forward propagation\n",
    "#     fit = loss(predicted_output, labels)  # calculate our measure of goodness\n",
    "#     fit.backward() # backpropagation\n",
    "#     optimizer.step() # update the weights of our trainable parameters\n",
    "#     train_loss += fit.item()\n",
    "#   train_loss = train_loss / len(trainDataLoader)\n",
    "#   train_loss_history += [train_loss]\n",
    "#   print(f'Epoch {epoch}, Train loss {train_loss}')\n",
    "\n",
    "\n",
    "for epoch in range(20):\n",
    "  train_loss = 0.0\n",
    "  test_loss = 0.0\n",
    "\n",
    "  model.train()\n",
    "  for i, data in enumerate(zip(trainDataLoader, trainDataLoaderLabels)):\n",
    "    images, labels = data\n",
    "    images = images.cuda()\n",
    "    labels = labels.cuda()\n",
    "    optimizer.zero_grad() # zero out any gradient values from the previous iteration\n",
    "    predicted_output = model(images) # forward propagation\n",
    "    fit = loss(predicted_output, labels)  # calculate our measure of goodness\n",
    "    fit.backward() # backpropagation\n",
    "    optimizer.step() # update the weights of our trainable parameters\n",
    "    train_loss += fit.item()\n",
    "\n",
    "  model.eval()\n",
    "  for i, data in  enumerate(zip(testDataLoader, testDataLoaderLabels)):\n",
    "    with torch.no_grad():\n",
    "      images, labels = data\n",
    "      images = images.cuda()\n",
    "      labels = labels.cuda()\n",
    "      predicted_output = model(images)\n",
    "      fit = loss(predicted_output, labels)\n",
    "      test_loss += fit.item()\n",
    "      _, predicted = torch.max(predicted_output.data, 1)\n",
    "      matches += (predicted == labels).sum().item()\n",
    "      total += labels.size(0)\n",
    "  train_loss = train_loss / len(trainDataLoader)\n",
    "  test_loss = test_loss / len(testDataLoader)\n",
    "  train_loss_history += [train_loss]\n",
    "  test_loss_history += [test_loss]\n",
    "  print(f'Epoch {epoch}, Train loss {train_loss}, Test loss {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6ebe812-bb30-4d4d-81c4-974880503ff8",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Preprocess the image if necessary\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Perform inference\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Here we assume 'resnet' is already loaded and 'image' is preprocessed\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m output \u001b[38;5;241m=\u001b[39m model(image)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Perform post-processing if necessary\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Append the prediction to the predictions list\u001b[39;00m\n\u001b[1;32m     27\u001b[0m predictions\u001b[38;5;241m.\u001b[39mappend(output\u001b[38;5;241m.\u001b[39margmax()\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 97\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 97\u001b[0m     out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)))\n\u001b[1;32m     98\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(out)\n\u001b[1;32m     99\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(out)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    457\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "test_images_nl = unpickle('cifar_test_nolabels.pkl')[b'data']\n",
    "\n",
    "test_images_nl = test_images_nl.reshape((-1, 3, 32, 32)).astype(np.float32) / 255.0\n",
    "\n",
    "test_images_tensor_nl = torch.tensor(test_images_nl, dtype=torch.float32)\n",
    "\n",
    "testDataLoaderNL = torch.utils.data.DataLoader(test_images_tensor_nl, batch_size=64,shuffle=True)\n",
    "\n",
    "\n",
    "predictions = []\n",
    "for image in testDataLoaderNL:\n",
    "    images = images.cuda()\n",
    "    # Preprocess the image if necessary\n",
    "    # Perform inference\n",
    "    # Here we assume 'resnet' is already loaded and 'image' is preprocessed\n",
    "    output = model(image)\n",
    "    # Perform post-processing if necessary\n",
    "    # Append the prediction to the predictions list\n",
    "    predictions.append(output.argmax().item())\n",
    "\n",
    "print(predictions)\n",
    "\n",
    "# Get predictions from your model\n",
    "# labels = model.predict(images)\n",
    "\n",
    "# # Create a DataFrame with IDs and labels\n",
    "# df = pd.DataFrame({\n",
    "#     'ID': image_ids,\n",
    "#     'Labels': labels\n",
    "# })\n",
    "\n",
    "# # Save the DataFrame to a CSV file\n",
    "# df.to_csv('predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af5ad9e-5de1-4d90-837d-c34c33087e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "torch.manual_seed(1024)\n",
    "\n",
    "def load_cifar_batch(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        batch = pickle.load(fo, encoding='bytes')\n",
    "    return batch\n",
    "\n",
    "# def load_cifar_batches(files):\n",
    "#     all_images = []\n",
    "#     all_labels = []\n",
    "#     for file in files:\n",
    "#         with open(file, 'rb') as fo:\n",
    "#             batch = pickle.load(fo, encoding='bytes')\n",
    "#         all_images.append(batch[b'data'].reshape((-1, 3, 32, 32)).astype(np.float32) / 255.0)\n",
    "#         all_labels.append(batch[b'labels'])\n",
    "#     return np.concatenate(all_images), np.concatenate(all_labels)\n",
    "\n",
    "def load_cifar_batch(file, transform=None):\n",
    "    with open(file, 'rb') as fo:\n",
    "        batch = pickle.load(fo, encoding='bytes')\n",
    "    images = batch[b'data']\n",
    "    if len(batch) > 2:\n",
    "        labels = batch[b'labels']\n",
    "    num_images = images.shape[0]\n",
    "    \n",
    "    # Reshape the images into the correct format: NxCxHxW\n",
    "    images = images.reshape((num_images, 3, 32, 32)).astype(np.float32) / 255.0\n",
    "    \n",
    "    # Convert images to a tensor\n",
    "    images_tensor = torch.tensor(images, dtype=torch.float32)\n",
    "    \n",
    "    # Apply transformations if any\n",
    "    if transform is not None:\n",
    "        images_tensor = transform(images_tensor)\n",
    "    if len(batch) > 2:\n",
    "        return images_tensor, torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return image_tensor\n",
    "\n",
    "\n",
    "# train_transform = transforms.Compose([\n",
    "#     # transforms.RandomCrop(32, padding=4), # Apply random crops\n",
    "#     # transforms.ToTensor(), # Convert images to PyTorch tensors\n",
    "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Normalize images\n",
    "#     # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "# ])\n",
    "\n",
    "# test_transform = transforms.Compose([\n",
    "#     # transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "#     # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "# ])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# transformation for train data \n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(10), # Add random rotation with 10 degrees\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1), # Add color jitter\n",
    "    transforms.RandomHorizontalFlip(), #apply horizontal flipping\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# batch_files = ['cifar-10-batches-py/data_batch_1', 'cifar-10-batches-py/data_batch_2', 'cifar-10-batches-py/data_batch_3', 'cifar-10-batches-py/data_batch_4', 'cifar-10-batches-py/data_batch_5']\n",
    "# train_images_dict, train_labels = load_cifar_batches(batch_files)\n",
    "\n",
    "# # print(len(train_images_dict))\n",
    "# train_images = train_images_dict.reshape((-1, 3, 32, 32)).astype(np.float32) / 255.0 \n",
    "# # train_labels = train_labels_dict[b'labels']\n",
    "\n",
    "# train_images_tensor = torch.tensor(train_images, dtype=torch.float32)\n",
    "# train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "\n",
    "# trainDataLoader = torch.utils.data.DataLoader(train_images_tensor, batch_size=64,shuffle=True)\n",
    "# trainDataLoaderLabels = torch.utils.data.DataLoader(train_labels_tensor, batch_size=64,shuffle=True)\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~\n",
    "\n",
    "# batch_1_dict = load_cifar_batch('cifar-10-batches-py/data_batch_3')\n",
    "# train_images = batch_1_dict[b'data'].reshape((-1, 3, 32, 32)).astype(np.float32) / 255.0 \n",
    "# train_labels = batch_1_dict[b'labels']\n",
    "\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~\n",
    "\n",
    "# batch_files = ['cifar-10-batches-py/data_batch_1', 'cifar-10-batches-py/data_batch_2', 'cifar-10-batches-py/data_batch_3', 'cifar-10-batches-py/data_batch_4', 'cifar-10-batches-py/data_batch_5']\n",
    "# trainarr = None\n",
    "# trainlabel = None\n",
    "\n",
    "# for file in batch_files:\n",
    "#     # Load CIFAR-10 batch\n",
    "#     batch_data = load_cifar_batch(file)\n",
    "    \n",
    "#     # Extract images and labels\n",
    "#     images = batch_data[b'data']  # Assuming 'data' contains the images\n",
    "#     labels = batch_data[b'labels']  # Assuming 'labels' contains the corresponding labels\n",
    "    \n",
    "#     # Reshape images to (-1, 3, 32, 32) and normalize\n",
    "#     images = images.reshape((-1, 3, 32, 32)).astype(np.float32) / 255.0\n",
    "    \n",
    "#     # Append images and labels to trainarr and trainlabel respectively\n",
    "#     if trainarr is None:\n",
    "#         trainarr = images\n",
    "#         trainlabel = labels\n",
    "#     else:\n",
    "#         trainarr = np.concatenate((trainarr, images), axis=0)\n",
    "#         trainlabel = np.concatenate((trainlabel, labels), axis=0)\n",
    "\n",
    "batch_files = ['cifar-10-batches-py/data_batch_1', 'cifar-10-batches-py/data_batch_2', 'cifar-10-batches-py/data_batch_3', 'cifar-10-batches-py/data_batch_4', 'cifar-10-batches-py/data_batch_5']\n",
    "trainarr = None\n",
    "trainlabel = None\n",
    "\n",
    "for file in batch_files:\n",
    "    # Load CIFAR-10 batch\n",
    "    batch_images, batch_labels = load_cifar_batch(file, transform=train_transform)\n",
    "    \n",
    "    # Reshape images to (-1, 3, 32, 32) and normalize\n",
    "    # batch_images = batch_images.reshape((-1, 3, 32, 32)).astype(np.float32) / 255.0\n",
    "    \n",
    "    # Append images and labels to trainarr and trainlabel respectively\n",
    "    if trainarr is None:\n",
    "        trainarr = batch_images\n",
    "        trainlabel = batch_labels\n",
    "    else:\n",
    "        trainarr = np.concatenate((trainarr, batch_images), axis=0)\n",
    "        trainlabel = np.concatenate((trainlabel, batch_labels), axis=0)\n",
    "\n",
    "\n",
    "train_images_tensor = torch.tensor(trainarr, dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(trainlabel, dtype=torch.long)\n",
    "\n",
    "\n",
    "trainDataLoader = torch.utils.data.DataLoader(train_images_tensor, batch_size=64,shuffle=True)\n",
    "trainDataLoaderLabels = torch.utils.data.DataLoader(train_labels_tensor, batch_size=64,shuffle=True)\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~\n",
    "\n",
    "\n",
    "\n",
    "# testdata = load_cifar_batch('cifar-10-batches-py/test_batch')\n",
    "# test_images = testdata[b'data'].reshape((-1, 3, 32, 32)).astype(np.float32) / 255.0 \n",
    "# test_labels = testdata[b'labels']\n",
    "\n",
    "test_images, test_labels = load_cifar_batch('cifar-10-batches-py/test_batch', transform=test_transform)\n",
    "\n",
    "# test_images = test_images.reshape((-1, 3, 32, 32)).astype(np.float32) / 255.0\n",
    "\n",
    "# test_images_tensor = torch.tensor(test_images, dtype=torch.float32)\n",
    "# test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "\n",
    "testDataLoader = torch.utils.data.DataLoader(test_images, batch_size=64,shuffle=True)\n",
    "testDataLoaderLabels = torch.utils.data.DataLoader(test_labels, batch_size=64,shuffle=True)\n",
    "\n",
    "\n",
    "images = next(iter(trainDataLoader))\n",
    "labels = next(iter(trainDataLoaderLabels))\n",
    "print(len(trainDataLoader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90918d08-41e3-49e7-a740-0943ac1b1e35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45731143-2c18-48d4-be6f-69dfb5210637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_cifar_batch(file, transform=None):\n",
    "#     with open(file, 'rb') as fo:\n",
    "#         batch = pickle.load(fo, encoding='bytes')\n",
    "#     images = batch[b'data']\n",
    "#     labels = batch[b'labels']\n",
    "#     num_images = images.shape[0]\n",
    "    \n",
    "#     # Reshape the images into the correct format: NxCxHxW\n",
    "#     images = images.reshape((num_images, 3, 32, 32)).astype(np.float32) / 255.0\n",
    "    \n",
    "#     # Convert images to a tensor\n",
    "#     images_tensor = torch.tensor(images, dtype=torch.float32)\n",
    "    \n",
    "#     # Apply transformations if any\n",
    "#     if transform is not None:\n",
    "#         images_tensor = transform(images_tensor)\n",
    "\n",
    "#     return images_tensor, torch.tensor(labels, dtype=torch.long)\n",
    "# # Define tensor transformations\n",
    "# tensor_transform = transforms.Compose([\n",
    "#     # Note: transforms for tensors here. Ensure they operate on tensors\n",
    "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "# ])\n",
    "\n",
    "# # Example of loading a batch with transformations\n",
    "# batch_images, batch_labels = load_cifar_batch('cifar-10-batches-py/data_batch_1', transform=tensor_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1444081d-cc18-427d-9c8b-23b1219106ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1662"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
