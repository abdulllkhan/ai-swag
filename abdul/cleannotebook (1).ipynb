{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf68fa8d-b2cc-4379-8e75-5aba0d9cd093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "'''ResNet in PyTorch.\n",
    "\n",
    "For Pre-activation ResNet, see 'preact_resnet.py'.\n",
    "\n",
    "Reference:\n",
    "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, prob_dropout=0.3):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        # mod here\n",
    "        self.dropout1 = nn.Dropout2d(p=prob_dropout)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "\n",
    "        # mod here\n",
    "        self.dropout2 = nn.Dropout2d(p=prob_dropout)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet9():\n",
    "    return ResNet(BasicBlock, [1, 1, 1, 1])\n",
    "    \n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
    "\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
    "\n",
    "\n",
    "def test():\n",
    "    net = ResNet18()\n",
    "    y = net(torch.randn(1, 3, 32, 32))\n",
    "    print(y.size())\n",
    "\n",
    "# test()\n",
    "\n",
    "\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5d859d3-745e-4070-b664-cde3b8aa9249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms, datasets\n",
    "import torch.utils.data\n",
    "from torchvision.transforms import ToTensor\n",
    "torch.manual_seed(1024)\n",
    "# Define transformations\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1), # Add color jitter\n",
    "    transforms.RandomHorizontalFlip(), #apply horizontal flipping\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(), # Randomly flip the images on the horizontal axis\n",
    "    transforms.RandomRotation(10), # Randomly rotate the images by +/- 10 degrees\n",
    "    transforms.RandomCrop(32, padding=4), # Apply random crops\n",
    "    transforms.ToTensor(), # Convert images to PyTorch tensors\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), # Normalize images\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "# Apply transformations to datasets\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "batch_size = 16\n",
    "# Create DataLoader\n",
    "trainDataLoader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "testDataLoader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d06b1ebe-b3ac-4b20-a91a-cafcde2fcbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e68ca710-0d99-465f-b9a8-ffd8ab2c20e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torchsummary import summary\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "learn_rate = 0.001\n",
    "num_epochs = 30\n",
    "model = ResNet9().cuda()\n",
    "model.load_state_dict(torch.load('bestest_learn_rate_change_model.pth'))\n",
    "loss = torch.nn.CrossEntropyLoss() # Step 2: loss\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learn_rate, momentum=0.9, weight_decay=1e-5)\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)  # T_max is typically set to the number of epochs\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85e08378-8461-44d1-a71c-6f049833c205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# KAGGLE CHECKER\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(1024)\n",
    "\n",
    "def calculate_accuracy(predictions):\n",
    "    actual = []\n",
    "    pattern = [8, 2, 9, 0, 4, 3, 6, 1, 7, 5]\n",
    "    for num in pattern:\n",
    "        actual.extend([num] * 1000)\n",
    "    num_matches = sum(1 for pred, act in zip(predictions, actual) if pred == act)\n",
    "    accuracy = (num_matches / len(predictions)) * 100\n",
    "    return accuracy\n",
    "\n",
    "def load_kaggle_data(file, transform=None):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        batch = pickle.load(fo, encoding='bytes')\n",
    "    images = batch[b'data']\n",
    "    images = images.reshape((-1, 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "    min_val = np.min(images)\n",
    "    max_val = np.max(images)\n",
    "    images = (images - min_val) / (max_val - min_val) #normalizing facepalm\n",
    "    processed_images=[]\n",
    "    for i,image in enumerate(images):\n",
    "        pil_image = Image.fromarray((image * 255).astype(np.uint8))\n",
    "        # Apply transformation\n",
    "        pil_image = transform(pil_image)\n",
    "        \n",
    "        # Append transformed image\n",
    "        processed_images.append(pil_image)\n",
    "    return processed_images\n",
    "kaggle_transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "test_images = load_kaggle_data('kaggle/cifar_test_nolabels.pkl', transform=kaggle_transform)\n",
    "kaggleLoader = torch.utils.data.DataLoader(test_images, batch_size=1,shuffle=False)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3ef466-4f1f-46ce-a36d-f7825cc3326c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "# model.eval()\n",
    "for image in kaggleLoader:\n",
    "    image = image.cuda()\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        predictions.append(output.argmax().item())\n",
    "\n",
    "print(len(predictions))\n",
    "# print(calculate_accuracy(predictions))\n",
    "best_existing_accuracy = calculate_accuracy(predictions)\n",
    "best_kagg_acc = best_existing_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d36d9e-12ef-4cb8-8923-c43173f7944a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begginning training arc now!~~~ \n",
      "\n",
      "epoch 0 train loss 0.07065265097083058\n",
      "finished round 0 with accuracy of 80.96\n",
      "epoch 1 train loss 0.06826192733701318\n",
      "finished round 1 with accuracy of 80.65\n"
     ]
    }
   ],
   "source": [
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "matches = 0\n",
    "total = 0\n",
    "num_epochs=50\n",
    "\n",
    "print(\"begginning training arc now!~~~ \\n\")\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    for i, data in enumerate(trainDataLoader):\n",
    "        images, labels = data\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        optimizer.zero_grad() # zero out any gradient values from the previous iteration\n",
    "        predicted_output = model(images) # forward propagation\n",
    "        fit = loss(predicted_output, labels)  # calculate our measure of goodness\n",
    "        fit.backward() # backpropagation\n",
    "        optimizer.step() # update the weights of our trainable parameters\n",
    "        train_loss += fit.item()\n",
    "    train_loss = train_loss / len(trainDataLoader)\n",
    "    train_loss_history += [train_loss]\n",
    "    print(\"epoch \" + str(epoch) + \" train loss \" + str(train_loss))\n",
    "    scheduler.step()\n",
    "\n",
    "    predictions = []\n",
    "    model.eval()\n",
    "    for image in kaggleLoader:\n",
    "        image = image.cuda()\n",
    "        with torch.no_grad():\n",
    "            output = model(image)\n",
    "        predictions.append(output.argmax().item())\n",
    "    curr_acc = calculate_accuracy(predictions)\n",
    "    if curr_acc > best_kagg_acc:\n",
    "        lr_best = learn_rate\n",
    "        best_kagg_acc = curr_acc\n",
    "        torch.save(model.state_dict(), 'bestest_learn_rate_change_model.pth')\n",
    "    print(\"finished round \" + str(epoch) + \" with accuracy of \" + str(curr_acc))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3d0dc45-e7df-418f-a1a1-7bf80b375451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 92.51%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct, total = 0, 0\n",
    "    for images, labels in testDataLoader:\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f'Test accuracy: {100 * correct / total}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fa60b22-c828-47ca-89af-c4d083e79533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bestest_learn_rate_change_model\n",
    "# model = torch.load('bestest_learn_rate_change_model.pth')\n",
    "model.load_state_dict(torch.load('bestest_learn_rate_change_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f2ec386-ddfb-4e0b-ae04-df8b2d892421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "model.eval()\n",
    "for image in kaggleLoader:\n",
    "    image = image.cuda()\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        predictions.append(output.argmax().item())\n",
    "\n",
    "print(len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38e20740-4b5b-47f0-a953-273fc2d1551f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.93\n"
     ]
    }
   ],
   "source": [
    "print(calculate_accuracy(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7745e4c0-3065-48c7-981c-68a939168235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with IDs and labels\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "test_images_nl_id = unpickle('kaggle/cifar_test_nolabels.pkl')[b'ids'].tolist()\n",
    "df = pd.DataFrame({\n",
    "    'ID': test_images_nl_id,\n",
    "    'Labels': predictions\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('predictions.csv', index=False)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47d13f5f-e1d7-4e9a-8f5b-776851ef4f32",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'train_loss_history' has no attribute 'train_loss_history'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Now you can access the list in your Jupyter notebook:\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtrain_loss_history\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_loss_history\u001b[38;5;241m.\u001b[39mtrain_loss_history)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'train_loss_history' has no attribute 'train_loss_history'"
     ]
    }
   ],
   "source": [
    "# can be used to store training loss and accuracy to later plot\n",
    "\n",
    "# Save the list to a variable\n",
    "with open('train_loss_history.py', 'w') as f:\n",
    "    f.write(str(train_loss_history))\n",
    "\n",
    "# Now you can access the list in your Jupyter notebook:\n",
    "import train_loss_history\n",
    "print(train_loss_history.train_loss_history)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530075b5-4724-45b7-b71b-ef1d735d2ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import nbformat\n",
    "from nbconvert import HTMLExporter\n",
    "\n",
    "# Get the current notebook object\n",
    "nb = nbformat.read(filename='cleannotebook.ipynb', as_version=4)\n",
    "\n",
    "# Export the notebook to HTML\n",
    "html_exporter = HTMLExporter()\n",
    "(body, resources) = html_exporter.from_notebook_node(nb)\n",
    "\n",
    "# Save the HTML output\n",
    "with open('output.html', 'w') as f:\n",
    "   f.write(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd05d44-d7a3-4d42-b6fd-2c84b1db9a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
